[ { "title": "Lessons Learned Implementing a Must Succeed Durable Functions Workflow", "url": "/lessons-learned-implementing-a-must-succeed-durable-functions-workflow/", "categories": "", "tags": "azure-functions, durable-functions", "date": "2022-08-09 00:00:00 -0400", "snippet": "Durable Functions is an extension for Azure Functions to help write stateful services in a stateless environment. Here’s a few thoughts for a successful production implementation of a must succeed workflow. (and by must succeed, I mean a workflow where failure results in a broken state)Consider Failures And How To Mitigate ThemOutages in dependent services can be short or lengthy. For transient issues or short outages building a retry policy into the orchestration may help, depending on your use case. However, at some point you may want to fail the entire orchestration when it seems retries will not fix it without something changing. Retrying activities is built into Durable Functions, see https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-error-handling#automatic-retry-on-failure.If the retry policy is exhausted and items are left in a broken state, now what? It can be helpful to consider this issue early and build for it. If you’re able to retry the entire orchestration from the beginning, creating a process to manually restart from the beginning may help. As part of this process you may want to retry all failed items for a datetime range and/or retry specific failed items. If you’re not able to retry the entire workflow from the beginning you may need to get more creative than simply running it again, but at least you’re thinking about it before it happens.On Failure ActivityIt can be helpful to have a graceful failure activity. This gives an opportunity to make any data updates, log additional info, etc.Using C# it could look like this:[FunctionName(\"MyProcess_Start\")]public async Task Start( [OrchestrationTrigger] IDurableOrchestrationContext context){ try { // Logic here, context.CallActivityWithRetryAsync calls that retry // will stay at the statement and throw if retry policy is exhausted. } catch { await context.CallActivityWithRetryAsync( \"MyProcess_Failed\", RetryOptions, // not defined here, specify your own (dataItem1, dataItem2)); }}[FunctionName(\"MyProcess_Failed\")]public async Task Failed( [ActivityTrigger] IDurableActivityContext inputs){ (string dataItem1, string dataItem2) = inputs.GetInput&lt;(string, string)&gt;(); // Run on failure logic here}InstrumentationIt can be helpful to view information about the state of each workflow item. Whether you store a viewable state on your own, use Application Insights, or inspect the underlying Durable Functions infrastructure, it may be useful to look at these items before you need to in response to an issue.Other PostsFor debugging: Thoughts On Debugging Durable Functions Extension Orchestrations For Azure Functions Backed By Azure Table StorageIf you’re building a fan-out operation you may find this post helpful (it may be difficult to tell which operation or operations are having issues): Debugging Fan-Out Operation Failures With Custom Exception Messages Using Durable Functions Extension For Azure Functions On C#" }, { "title": "Setting Regex Timeout Globally Using .NET 6.0 With C#", "url": "/setting-regex-timeout-globally-using-dotnet-6_0-with-csharp/", "categories": "", "tags": "dotnet", "date": "2022-04-22 00:00:00 -0400", "snippet": ".NET 6.0 allows the global regular expression (regex) timeout to be configured. By default, the global timeout is Regex.InfiniteMatchTimeout and regex operations will run until completion.If a timeout is exceeded System.Text.RegularExpressions.RegexMatchTimeoutException will be thrown.Setting the global regex timeoutThis will set the default regex timeout to 2 seconds:// https://learn.microsoft.com/en-us/dotnet/api/system.text.regularexpressions.regex.matchtimeout?view=net-6.0#remarksAppDomain.CurrentDomain.SetData(\"REGEX_DEFAULT_MATCH_TIMEOUT\", TimeSpan.FromSeconds(2));This needs to happen as early as possible (first statement in Main would work), prior to any usage of Regex that causes this value to be read and cached. Once it’s cached it cannot be changed without restarting the application. (see Runtime implementation below)We can verify it’s working via:var regexDefaultMatchTimeout = AppDomain.CurrentDomain.GetData(\"REGEX_DEFAULT_MATCH_TIMEOUT\") as TimeSpan?;if (regexDefaultMatchTimeout != null){ var regexMatchTimeout = new Regex(\"abc\").MatchTimeout; if (regexDefaultMatchTimeout != regexMatchTimeout) { throw new ApplicationException(\"MatchTimeout is not set from REGEX_DEFAULT_MATCH_TIMEOUT.\"); }}Console.WriteLine(regexDefaultMatchTimeout);Why set a global timeout?Regular expressions could be used by an attacker to launch a denial-of-service attack for a website by consuming excessive resources. Setting a timeout allows the operation to stop at a configured timeout, rather than running until completion, using resources the entire time.Application code / library codeIf you’re writing application code be careful with negatively impacting dependencies via a regex timeout.If you’re writing library code to be used by other applications and want a specific regex timeout it should probably be handled internally by manually specifying the timeout values for each regex call, rather than using a default value.Example:Regex.IsMatch(\"abc\", \"a\", RegexOptions.None, TimeSpan.FromSeconds(5));Runtime implementationHere’s the .NET 6.0 implementation: https://github.com/dotnet/runtime/blob/release/6.0/src/libraries/System.Text.RegularExpressions/src/System/Text/RegularExpressions/Regex.Timeout.cs#L31" }, { "title": "Retrieving ObjectIds In An Azure Active Directory Group Using Azure CLI", "url": "/retrieving-objectids-in-an-azure-active-diretory-group-using-azure-cli/", "categories": "", "tags": "azure, azure-cli", "date": "2021-07-02 00:00:00 -0400", "snippet": "Using Azure CLI we can retrieve a list of objectIds in a Azure Active Directory group.SetupInstall the Azure CLI if it’s not already installed.If you aren’t currently logged in with Azure CLI run:&gt; az loginRetrieving ObjectIdsNow, you’re ready to get the objectIds!&gt; az ad group member list --group \"GROUP_NAME_HERE\" --query \"[].{displayName: displayName, userPrincipalName: userPrincipalName, objectId: objectId}\" --output tableDisplayName UserPrincipalName ObjectId----------- ----------------- ------------------------------------Test User 1 test1@example.com 00000000-0000-0000-0000-000000000001Test User 2 test2@example.com 00000000-0000-0000-0000-000000000002If you’re looking for a comma separated value:&gt; az ad group member list --group \"GROUP_NAME_HERE\" --query \"[].objectId | {objectIds: join(',', @)}\"{ \"objectIds\": \"00000000-0000-0000-0000-000000000001,00000000-0000-0000-0000-000000000002\"}" }, { "title": "Naming Durable Functions With Prefixes For Application Insights Filtering", "url": "/naming-durable-functions-with-prefixes-for-application-insights-filtering/", "categories": "", "tags": "azure-functions, durable-functions", "date": "2021-04-23 00:00:00 -0400", "snippet": "Durable Functions is an extension for Azure Functions to help write stateful services in a stateless environment. After deploying Durable Functions functionality to an existing Azure Functions application you may find the average response time in Application Insights appears degraded. If you want to verify performance metrics haven’t degraded in a user-facing manner it may be helpful to filter out Durable Functions items from Application Insights.Application Insights QueryDurable Functions with a consistent prefix can be filtered out from Application Insights queries with a single where statement.| where operation_Name !startswith \"MyPrefix_\"A full example might look like:let start=datetime(\"2021-04-22T00:00:00.000Z\");let end=datetime(\"2021-04-23T00:00:00.000Z\");let timeGrain=5m;let dataset=requests | where timestamp &gt; start and timestamp &lt; end | where client_Type != \"Browser\" | where operation_Name !startswith \"MyPrefix_\";dataset| summarize avg(duration) by bin(timestamp, timeGrain)| extend request='Overall'| render timechartC# ExampleThis is a C# .NET Core 3.1 example. Apply the same idea to any other supported version or language.[FunctionName(\"Initiate\")]public async Task&lt;IActionResult&gt; Initiate( [HttpTrigger(AuthorizationLevel.User, \"post\", Route = \"initiate\")] HttpRequestMessage req, [DurableClient] IDurableOrchestrationClient client){ await client.StartNewAsync( \"MyPrefix_RunProcess\", new { Arg1 = \"Arg1-value\", Arg2 = \"Arg2-value\" }); return new AcceptedResult();}[FunctionName(\"MyPrefix_RunProcess\")]public async Task MyPrefix_RunProcess([OrchestrationTrigger] IDurableOrchestrationContext context){ var input = context.GetInput&lt;JObject&gt;(); var arg1 = input[\"Arg1\"].Value&lt;string&gt;(); var arg2 = input[\"Arg2\"].Value&lt;string&gt;(); await context.CallActivityAsync(\"MyPrefix_Activity1\", (arg1, arg2)); await context.CallActivityAsync(\"MyPrefix_Activity2\", arg1);}[FunctionName(\"MyPrefix_Activity1\")]public async Task Activity1([ActivityTrigger] IDurableActivityContext inputs){ (var arg1, var arg2) = inputs.GetInput&lt;(string, string)&gt;(); await Task.Delay(TimeSpan.FromSeconds(30));}[FunctionName(\"MyPrefix_Activity2\")]public async Task Activity2([ActivityTrigger] IDurableActivityContext inputs){ var arg1 = inputs.GetInput&lt;string&gt;(); await Task.Delay(TimeSpan.FromSeconds(15));}" }, { "title": "Retrieving All Managed Identity ObjectIds In Resource Group For Azure App Services And Azure Functions Using Azure CLI", "url": "/retrieving-all-managed-identity-objectids-in-resource-group-for-azure-app-services-and-azure-functions-using-azure-cli/", "categories": "", "tags": "azure, azure-cli", "date": "2021-04-20 00:00:00 -0400", "snippet": "If you’re working with Azure App Services, Azure Functions, or perhaps another Azure service that implements managed identity in the same way it can be useful to get a list of the object ids for an entire resource group. One way we can accomplish this is using the Azure CLI.SetupInstall the Azure CLI if it’s not already installed.If you aren’t currently logged in with Azure CLI run:&gt; az loginNext, switch to the subscription you want to target. Alternatively, pass the --subscription parameter as needed when retrieving objectIds.&gt; az account set --subscription name_or_id_of_subscriptionRetrieving ObjectIdsNow, you’re ready to get the objectIds!&gt; az resource list -g myresourcegroup --query \"[?identity!=null].{name: name, objectIds: identity.principalId}\" --output tableName ObjectIds------------------------ ------------------------------------myservice-eastus 00000000-0000-0000-0000-000000000001myservice-westus 00000000-0000-0000-0000-000000000003myservice-eastus/staging 00000000-0000-0000-0000-000000000002myservice-westus/staging 00000000-0000-0000-0000-000000000004If you’re looking for a comma separated value:&gt; az resource list -g myresourcegroup --query \"[?identity!=null].identity.principalId | {objectIds: join(',', @)}\"{ \"objectIds\": \"00000000-0000-0000-0000-000000000001,00000000-0000-0000-0000-000000000003,00000000-0000-0000-0000-000000000002,00000000-0000-0000-0000-000000000004\"}" }, { "title": "Thoughts On Debugging Durable Functions Extension Orchestrations For Azure Functions Backed By Azure Table Storage", "url": "/thoughts-on-debugging-durable-functions-extension-orchestrations-for-azure-functions-backed-by-azure-table-storage/", "categories": "", "tags": "azure-functions, durable-functions", "date": "2021-03-31 00:00:00 -0400", "snippet": "Durable Functions is an extension for Azure Functions to help write stateful services in a stateless environment. Here’s a few thoughts I’ve compiled for debugging Durable Functions orchestrations.Note: This post assumes Azure Table Storage is the backend data store.Finding The InstanceIdA good first step is finding the InstanceId if you don’t already have it. This is the PartitionKey for table storage which can filter the tables for this specific instance.In table storage you’ll find two tables, one ending in History and one ending in Instances. The Instances can be helpful for determining the InstanceId, as the InstanceId is the PartitionKey in table storage and it contains the input parameters for each instance.Note: If multiple Azure Functions applications are running the same code but using their own backend state storage for Durable Functions you’ll need to find which web app is handling the Durable Functions instance. You can guess and check Table Storage or query Application Insights for the appName using an Instance summary query.ToolingHere’s a few options: View Table Storage directly: A quick option is browsing to table storage directly via the Azure portal. Navigate the storage account on the Azure portal, open Storage Explorer on the left pane, expand Tables, view the History and Instance tables filtering by PartitionKey as the InstanceId where helpful. Durable Functions Monitor: Per the repository README.md: “A monitoring/debugging UI tool for Azure Durable Functions”. There’s different options for running it, including a Visual Studio Code extension. Azure Functions Core Tools: If you prefer the command line check out this docs page, noting the “Azure Functions Core Tools” headings: Manage instances in Durable Functions in Azure. Run the commands from the root folder of the Azure Functions application. For non-local debugging you’ll need to update the storage connection string. By default, it uses the AzureWebJobsStorage storage connection string but can be overridden. Application Insights: If data is being logged to Application Insights you can run queries. For example, Instance summary query “… displays the status of all orchestration instances that were run in a specified time range.” which you may find useful if you’re looking for a high level view, which can help bridge between multiple Azure Functions applications running the same code.Thoughts For Future DebuggingAfter you’ve worked through an issue you have an opportunity to potentially make a similar investigation easier next time. Would setting a Custom Status help? If it’s a fan-out/fan-in operation would enhancing exception messages with the HTTP verb, url, and information about the response help (HTTP status code or perhaps even the response body, depending on confidentiality concerns with logging the response content)? You may find my Debugging Fan-Out Operation Failures With Custom Exception Messages Using Durable Functions Extension For Azure Functions On C# post useful if that sounds helpful.Hope this helps if you’re investigating an issue!" }, { "title": "Running Code On Local Computer / Localhost Only Using Azure Functions With C#", "url": "/running-code-on-local-computer-localhost-only-using-azure-functions-with-csharp/", "categories": "", "tags": "azure-functions, csharp", "date": "2021-03-19 00:00:00 -0400", "snippet": "Azure Functions is a serverless hosting offering available in Microsoft Azure. While authoring code you may find a need to run code only during local development.Azure Functions provides an AZURE_FUNCTIONS_ENVIRONMENT environment variable that can help! (Note: Per the documentation it seems to be a version 2.x and later feature, I couldn’t find a reference to it in https://github.com/Azure/azure-functions-host/tree/v1.x)var isLocal = Environment .GetEnvironmentVariable(\"AZURE_FUNCTIONS_ENVIRONMENT\") ?.Equals(\"development\", StringComparison.OrdinalIgnoreCase) == true;This is particularly useful over the #if DEBUG preprocessor directive if you want the code to run the same as local when running on a build agent in a Release configuration.Hope that helps!" }, { "title": "Debugging Fan-Out Operation Failures With Custom Exception Messages Using Durable Functions Extension For Azure Functions On C#", "url": "/debugging-fan-out-operation-failures-with-custom-exception-messages-using-durable-functions-extension-for-azure-functions-on-csharp/", "categories": "", "tags": "azure-functions, durable-functions", "date": "2021-03-16 00:00:00 -0400", "snippet": "Durable Functions is an extension for Azure Functions to help write stateful services in a stateless environment. There’s an orchestration pattern named Fan out/fan in that can be helpful for contacting a number of external services and waiting for their replies. When one or more of these services throws an exception (for instance, calling EnsureSuccessStatusCode() on the response) it can be difficult to determine which service or services are having problems.See the following:var serviceUrl = \"https://example.com\";var httpClient = httpClientFactory.CreateClient();var response = await httpClient.GetAsync(serviceUrl);// Example: \"Response status code does not indicate success: 500 (Internal Server Error).\"// can be difficult to debug since it doesn't indicate the url for the service.response.EnsureSuccessStatusCode();Instead of EnsureSuccessStatusCode() change it to the code below:var serviceUrl = \"https://example.com\";var httpClient = httpClientFactory.CreateClient();var response = await httpClient.GetAsync(serviceUrl);if (!response.IsSuccessStatusCode){ // Example: \"GET https://example.com returned non-success HTTP status code: 500 InternalServerError.\" throw new HttpRequestException( $\"{request.Method} {request.RequestUri} returned non-success HTTP status code: {(int)response.StatusCode} {response.StatusCode}.\");}This can help identify the service or services that are having an issue when debugging issues with Durable Functions." }, { "title": "Avoid Exposing Status Uri From Durable Functions Extension For Azure Functions To Untrusted Parties (It Contains A Secret Key!)", "url": "/avoid-exposing-status-uri-from-durable-functions-extension-for-azure-functions-to-untrusted-parties-it-contains-a-secret-key/", "categories": "", "tags": "azure-functions, durable-functions", "date": "2021-03-15 00:00:00 -0400", "snippet": "Durable Functions is an extension for Azure Functions to help write stateful services in a stateless environment. It can return helpful information, including a status uri, but this contains a secret key by default! Be careful not to expose this to untrusted parties./runtime/webhooks/durabletask/instances/abc123abc123abc123abc123abc123ab?taskHub=mytaskhub&amp;connection=Storage&amp;code=code-here ends with a code query string paramater containing the secret key.Consider building a custom status endpoint, securing it as needed.If Deployed, Rotate The Key(s)!The code appended to the url as a query string parameter is a system key that enables access to Durable Functions at an administrative level for the Azure Functions instance. You’ll want to rotate the impacted any durabletask_extension system keys in your Azure Functions applications.More Info https://learn.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-instance-management?tabs=csharp#retrieve-http-management-webhook-urls https://github.com/Azure/azure-functions-durable-extension/blob/165159e22eaa9bf4039ab6ef292311e6a58cb7c8/src/WebJobs.Extensions.DurableTask/HttpApiHandler.cs#L1059-L1104 includes a comment that a systemKey will be added." }, { "title": "IDisposable, IAsyncDisposable, and Tasks: Dispose Behavior With Tasks In C#", "url": "/idisposable-iasyncdisposable-and-tasks-dispose-behavior-with-tasks-in-csharp/", "categories": "", "tags": "dotnet", "date": "2021-02-10 00:00:00 -0500", "snippet": "While working with HTTP calls that return a Stream via HttpClient’s var response = await SendAsync(request, HttpCompletionOption.ResponseHeadersRead); -&gt; var stream = await response.Content.ReadAsStreamAsync(); method for efficient handling of streamed data Brian Dunnington identified the response should be disposed and pointed me to https://www.stevejgordon.co.uk/using-httpcompletionoption-responseheadersread-to-improve-httpclient-performance-dotnet.Working through it I found disposing a Task&lt;T&gt; itself doesn’t appear to call underlying Dispose/DisposeAsync methods on the generic T. Instead, the T itself, in this case a Stream, should be disposed.Also, https://devblogs.microsoft.com/pfxteam/do-i-need-to-dispose-of-tasks/ indicates that disposing of tasks is probably not necessary, but rather something to consider to meet performance goals. For what it’s worth this post is dated in 2012, prior to the release of .NET Core.Exampleusing System;using System.Threading.Tasks; public class Program{ public static async Task Main() { // // IDisposable // var myDisposableTask = Task.FromResult(new MyDisposable()); myDisposableTask.Dispose(); MyDisposable myDisposable; using (myDisposable = await myDisposableTask) { Console.WriteLine(\"myDisposable.IsDisposed inside using: \" + myDisposable.IsDisposed /* == False */); }; Console.WriteLine(\"myDisposable.IsDisposed outside using: \" + myDisposable.IsDisposed /* == True */); // // IAsyncDisposable // var myAsyncDisposableTask = Task.FromResult(new MyAsyncDisposable()); myAsyncDisposableTask.Dispose(); MyAsyncDisposable myAsyncDisposable; await using (myAsyncDisposable = await myAsyncDisposableTask) { Console.WriteLine(\"myAsyncDisposable.IsDisposed inside await using: \" + myAsyncDisposable.IsDisposed /* == False */); } Console.WriteLine(\"myAsyncDisposable.IsDisposed outside await using: \" + myAsyncDisposable.IsDisposed /* == True */); }}public class MyDisposable : IDisposable{ public bool IsDisposed { get; private set; } public void Dispose() { IsDisposed = true; }}public class MyAsyncDisposable : IAsyncDisposable{ public bool IsDisposed { get; private set; } public ValueTask DisposeAsync() { IsDisposed = true; return ValueTask.CompletedTask; }}" }, { "title": "Retrieving Managed Identity Access Tokens For Azure Web Apps Using PowerShell Console", "url": "/retrieving-managed-identity-access-tokens-for-azure-web-apps-using-powershell-console/", "categories": "", "tags": "azure, powershell", "date": "2021-01-29 00:00:00 -0500", "snippet": "If you’re working with Azure Web Apps and securing between services using managed identity, formerly known as Managed Service Identity (MSI), it can be useful to retrieve an access token on behalf of an application. We can accomplish this via PowerShell.CreditThis is adapted from https://techcommunity.microsoft.com/t5/azure-developer-community-blog/understanding-azure-msi-managed-service-identity-tokens-caching/ba-p/337406 authored by Stephane Eyskens (@stephaneeyskens), big thanks for sharing this!CodeUpdate the resource of https://management.azure.com/ as needed for your use case and paste into a PowerShell prompt:$ProgressPreference=\"SilentlyContinue\"$secret = (get-item env:MSI_SECRET).value$endpoint = (get-item env:MSI_ENDPOINT).value$headers = @{Secret = \"$($secret)\"}$req=Invoke-WebRequest -UseBasicParsing -Uri \"$($endpoint)?api-version=2017-09-01&amp;resource=https://management.azure.com/\" -Headers $headers|ConvertFrom-JSON$req$req.access_tokenYou can access a PowerShell prompt via https://your-app-name.scm.azurewebsites.net/DebugConsole/?shell=powershell (replace your-app-name)." }, { "title": "Prevent Duplicate Invocations of Durable Functions Using Azure Functions With C#", "url": "/prevent-duplicate-invocations-of-durable-functions-using-azure-functions-with-csharp/", "categories": "", "tags": "azure-functions, durable-functions, dotnet", "date": "2021-01-15 00:00:00 -0500", "snippet": "Durable Functions is an extension for Azure Functions to help write stateful services in a stateless environment. When starting a new process, you may want to verify it isn’t already running by checking the list of in-flight Durable Functions with some custom logic.Durable Functions includes functionality to Query all instances which can help us.Note: With many in-flight Durable Functions this approach could be problematic by needing to query many pages of results.CodeHere’s a starting point to work from, copy and edit as needed:using Microsoft.Azure.WebJobs;using Microsoft.Azure.WebJobs.Extensions.DurableTask;using Microsoft.Azure.WebJobs.Extensions.Http;using System.Collections.Generic;using System.Linq;using System.Net;using System.Net.Http;using System.Threading;using System.Threading.Tasks;namespace MyApplication{ public class Api { [FunctionName(\"Initiate\")] public async Task&lt;HttpResponseMessage&gt; Initiate( [HttpTrigger(AuthorizationLevel.User, \"post\", Route = \"my/route\")] HttpRequestMessage req, [DurableClient] IDurableOrchestrationClient client) { string listContinuationToken = null; do { var currentInstances = await client.ListInstancesAsync( new OrchestrationStatusQueryCondition { ContinuationToken = listContinuationToken, RuntimeStatus = new[] { OrchestrationRuntimeStatus.ContinuedAsNew, OrchestrationRuntimeStatus.Pending, OrchestrationRuntimeStatus.Running } }, CancellationToken.None); var alreadyRunning = currentInstances.DurableOrchestrationState .Any(x =&gt; /* Logic here */); if (alreadyRunning) { return new HttpResponseMessage(HttpStatusCode.BadRequest) { Content = new StringContent(\"This process is currently running.\") }; } listContinuationToken = currentInstances.ContinuationToken; } while (listContinuationToken != null); var instanceId = await client.StartNewAsync(\"BeginProcess\"); return client.CreateCheckStatusResponse(req, instanceId); } [FunctionName(\"BeginProcess\")] public async Task&lt;IEnumerable&lt;string&gt;&gt; BeginProcess( [OrchestrationTrigger] IDurableOrchestrationContext context) { // Code here } }}" }, { "title": "Get Basic Information For Azure AD Object Ids Using PowerShell", "url": "/get-basic-information-for-azure-ad-object-ids-using-powershell/", "categories": "", "tags": "azure, powershell", "date": "2021-01-07 00:00:00 -0500", "snippet": "Quick summary&gt; Get-AzureADObjectByObjectId -ObjectIds 00000000-0000-0000-0000-000000000000Comma delimit for multiples.DetailsIf you’re working with Azure you may find yourself needing to get information about one or many Azure AD object ids. Here’s how to get basic information without having to specify the type of the entity: Install Azure Active Directory PowerShell for Graph if it’s not already installed. Install-Module AzureAD uses the PowerShell Gallery to install the AzureAD module. Connect via Connect-AzureAD. Note: The terminal in Visual Studio Code may have an issue here, if it’s not loading the login screen try launching standalone PowerShell. Use Get-AzureADObjectByObjectId -ObjectIds 00000000-0000-0000-0000-000000000000 where 00000000-0000-0000-0000-000000000000 is an example object id (replace it with your own, to add multiples delimit with a comma like -ObjectIds abc,def) Hope that helps!" }, { "title": "Reverting Azure ARM Template Changes Using Change History And Manual ARM Deployment", "url": "/reverting-azure-arm-template-changes-using-change-history-and-manual-arm-deployment/", "categories": "", "tags": "azure", "date": "2021-01-05 00:00:00 -0500", "snippet": "I made some ARM templates changes and unintentionally removed Access Policy items from an Azure Key Vault in a (thankfully) non-production environment. Since the ARM template doesn’t have the before state in the repository, reverting the pull request isn’t going to fix it.However, we can get the before state of the template using Change history on the Activity log viewable on the Azure portal.Change historyWe can use Change history to see the before state of the ARM template. Copy paste the before state into a code editor. You may find it useful to run a ARM template deployment what-if operation to verify the template before proceeding. You may need to work with it a bit to get it in a working state.Applying ARM templateTo manually publish the ARM template to Azure, see The “Edit and deploy the template” anchor on “Quickstart: Create and deploy ARM templates by using the Azure portal”.In ClosingI hope this is helpful if you find yourself needing to revert like I did." }, { "title": "SQL ADO.NET Scaler And Reader Sync vs Async Benchmarks Using BenchmarkDotNet, LocalDB, And C#", "url": "/sql-adonet-scaler-and-reader-sync-vs-async-benchmarks-using-benchmarkdotnet-localdb-and-csharp/", "categories": "", "tags": "dotnet", "date": "2020-12-18 00:00:00 -0500", "snippet": "I wrote some benchmarks using BenchmarkDotNet for ADO.NET C# code targeting LocalDB. Here’s what I found:ResultsBenchmarkDotNet=v0.12.1, OS=Windows 10.0.19042Intel Core i7-1065G7 CPU 1.30GHz, 1 CPU, 8 logical and 4 physical cores.NET Core SDK=5.0.101 [Host] : .NET Core 5.0.1 (CoreCLR 5.0.120.57516, CoreFX 5.0.120.57516), X64 RyuJIT DefaultJob : .NET Core 5.0.1 (CoreCLR 5.0.120.57516, CoreFX 5.0.120.57516), X64 RyuJIT Method Mean Error StdDev Gen 0 Gen 1 Gen 2 Allocated LocalDb_ExecuteScaler_Sync 99.41 μs 1.034 μs 0.917 μs 0.6104 0.1221 - 2.72 KB LocalDb_ExecuteScaler_AsyncConnection 99.40 μs 1.313 μs 1.025 μs 0.6104 0.1221 - 2.95 KB LocalDb_ExecuteScaler_AsyncConnectionAndAsyncExecuteScaler 151.66 μs 1.011 μs 0.896 μs 1.2207 - - 5.14 KB LocalDb_SqlDataReader_AllSync 74.57 μs 0.670 μs 0.559 μs 0.6104 0.1221 - 2.79 KB LocalDb_SqlDataReader_AsyncConnection_OneRow 71.29 μs 0.612 μs 0.511 μs 0.7324 0.1221 - 3.02 KB LocalDb_SqlDataReader_AsyncConnectionAndAsyncExecuteReader_OneRow 113.33 μs 1.427 μs 1.191 μs 1.0986 0.1221 - 4.53 KB LocalDb_SqlDataReader_AsyncConnectionAndAsyncExecuteReaderAndAsyncRead_OneRow 113.29 μs 1.876 μs 1.755 μs 1.2207 - - 5.13 KB LocalDb_SqlDataReader_AsyncConnection_OneThousandRows 271.60 μs 3.893 μs 3.251 μs 21.4844 - - 88.91 KB LocalDb_SqlDataReader_AsyncConnectionAndAsyncExecuteReader_OneThousandRows 326.75 μs 1.353 μs 1.266 μs 21.9727 - - 90.43 KB LocalDb_SqlDataReader_AsyncConnectionAndAsyncExecuteReaderAndAsyncRead_OneThousandRows 400.97 μs 2.214 μs 1.849 μs 36.6211 - - 148.79 KB LocalDb_SqlDataReader_AsyncConnection_OneMillionRows 162,186.81 μs 2,322.878 μs 2,172.821 μs 21000.0000 - - 85993.45 KB LocalDb_SqlDataReader_AsyncConnectionAndAsyncExecuteReader_OneMillionRows 163,133.35 μs 2,100.990 μs 1,862.473 μs 21000.0000 - - 85994.9 KB LocalDb_SqlDataReader_AsyncConnectionAndAsyncExecuteReaderAndAsyncRead_OneMillionRows 259,737.34 μs 5,034.817 μs 4,463.234 μs 35000.0000 - - 146159.24 KB An interesting finding here is awaiting ReadAsync() on SqlDataReader significantly impacts performance and greatly increases allocations over the sync Read() (at least, it’s noticable with 1,000 or 1,000,000 rows).CodeThe code for these benchmarks is at https://github.com/kendaleiv/CSharpSqlBenchmarks." }, { "title": "Monitoring Web Application Deployment Impact Using Artillery", "url": "/monitoring-web-application-deployment-impact-using-artillery/", "categories": "", "tags": "web", "date": "2020-12-16 00:00:00 -0500", "snippet": "If you’re looking to monitor the state of your web application during a deployment it can be useful to run a bunch of requests to see the impact during deployment. We can use a load testing tool, like Artillery, to accomplish this.Using ArtilleryArtillery can be configured via test scripts, but it can also use command line arguments for simple scenarios. You’ll need Node.js installed to run Artillery. From the command line you can test the endpoint for 10 requests per second (rate) for 10 minutes (duration in seconds):npx artillery quick --rate=10 --duration=600 https://example.org/healthArtillery provides an update every 10 seconds, and provides a summary at the end like this:Summary report @ 15:28:18(-0500) 2020-12-16 Scenarios launched: 6000 Scenarios completed: 6000 Requests completed: 6000 Mean response/sec: 9.99 Response time (msec): min: 77.2 max: 909.3 median: 98 p95: 158.5 p99: 279.7 Scenario counts: 0: 6000 (100%) Codes: 200: 6000If any non-success status codes were returned, like 500, they’d show up here. This means users could be impacted during the deployment.Global installIf you’re frequently running Artillery you may want to globally install it via npm install -g artillery, rather than use npx each time. After it’s globally installed you can omit npx from the above command." }, { "title": "Dynamic Parameterized SQL Using ADO.NET With C#", "url": "/dynamic-parameterized-sql-using-adonet-with-csharp/", "categories": "", "tags": "dotnet", "date": "2020-12-15 00:00:00 -0500", "snippet": "Here’s a C# solution for dynamic parameters inside a SQL string:var items = new[] { \"a\", \"b\", \"c\" };var connection = new SqliteConnection($\"Data Source={Filename}\");await connection.OpenAsync();var parameters = items.Select((x, i) =&gt; Tuple.Create($\"@{i}\", x));var command = connection.CreateCommand();command.CommandText = $\"select count(*) from TestTable where name in ({string.Join(\",\", parameters.Select(x =&gt; x.Item1))})\";foreach (var parameter in parameters){ command.Parameters.AddWithValue(parameter.Item1, parameter.Item2);}var count = (long)await command.ExecuteScalarAsync();Note: While it may be easier to inject the values directly into the string rather than constructing parameters, it’s a good idea to parameterize to prevent potential SQL injection attacks.Full sampleSee https://github.com/kendaleiv/CSharpDynamicSqlParameters for a full sample." }, { "title": "Limiting Concurrent Operations With SemaphoreSlim Using C#", "url": "/limiting-concurrent-operations-with-semaphoreslim-using-csharp/", "categories": "", "tags": "dotnet", "date": "2020-12-11 00:00:00 -0500", "snippet": "If you’re looking to limit the number of concurrent operations but maintain as high throughput as possible SemaphoreSlim can help! For instance, this could help maintain a consistent flow of HTTP requests to an external API during a bulk processing operation – respecting the limits of the external API to not potentially overwhelm it with too many concurrent requests.Exampleusing System.Threading;using System.Threading.Tasks;namespace MyApp{ public class MyService { private const int MaximumConcurrentOperations = 10; private static readonly SemaphoreSlim RateLimit = new SemaphoreSlim( initialCount: MaximumConcurrentOperations, maxCount: MaximumConcurrentOperations); public async Task Process() { await RateLimit.WaitAsync(); try { // Rate limited logic here } finally { RateLimit.Release(); } } }}maxCountIf maxCount is not specified int.MaxValue is used. From my understanding maxCount in this context is sort of a safety mechanism, making sure you aren’t releasing more than waiting.Consider the following example:using System;using System.Collections.Generic;using System.Threading;using System.Threading.Tasks;namespace MyApp{ class Program { static readonly SemaphoreSlim SemaphoreSlim = new SemaphoreSlim(initialCount: 1); static void Main(string[] args) { var tasks = new List&lt;Task&gt;(); for (var i = 0; i &lt; 10; i++) { tasks.Add(Task.Run(() =&gt; { SemaphoreSlim.Wait(); Console.WriteLine($\"Start task, CurrentCount: {SemaphoreSlim.CurrentCount}\"); Thread.Sleep(100); Console.WriteLine($\"End task, CurrentCount: {SemaphoreSlim.CurrentCount}\"); SemaphoreSlim.Release(); SemaphoreSlim.Release(); // Extra call to Release(); })); } Task.WaitAll(tasks.ToArray()); } }}With example output:Start task, CurrentCount: 0End task, CurrentCount: 0Start task, CurrentCount: 1Start task, CurrentCount: 0End task, CurrentCount: 0End task, CurrentCount: 0Start task, CurrentCount: 1Start task, CurrentCount: 0Start task, CurrentCount: 0Start task, CurrentCount: 1End task, CurrentCount: 0End task, CurrentCount: 0End task, CurrentCount: 0End task, CurrentCount: 0Start task, CurrentCount: 1Start task, CurrentCount: 0Start task, CurrentCount: 1End task, CurrentCount: 5End task, CurrentCount: 5End task, CurrentCount: 5However, if the SemaphoreSlim is changed to new SemaphoreSlim(initialCount: 1, maxCount: 1) then System.Threading.SemaphoreFullException is thrown instead.SummaryRate limiting using SemaphoreSlim can help avoid overwhelming external services with too many concurrent requests while maintaining high throughput." }, { "title": "Thread Safe Lazy Initialization Using C# Notes", "url": "/thread-safe-lazy-initialization-using-csharp-notes/", "categories": "", "tags": "dotnet", "date": "2020-11-23 00:00:00 -0500", "snippet": "Sometimes it makes sense to lazily initialize something in an application. In the case of an application internal cache waiting until the first access of data to prime the cache could improve startup times. One way of lazy initialization with C# is Lazy&lt;T&gt;.Lazy&lt;T&gt;Per https://learn.microsoft.com/en-us/dotnet/api/system.lazy-1#thread-safety the default is thread safe, but can be disabled. If you don’t opt-out it’s thread safe.using System;public class Program{ public static void Main() { // Uses LazyThreadSafetyMode.ExecutionAndPublication // See https://github.com/dotnet/corefx/blob/b8b81a66738bb10ef0790023598396861d92b2c4/src/Common/src/CoreLib/System/Lazy.cs#L240 var lazyString = new Lazy&lt;string&gt;(() =&gt; \"abc\"); Console.WriteLine(lazyString.Value); // abc }}Thread safety can be adjusted likevar lazyString = new Lazy&lt;string&gt;(() =&gt; \"abc\", isThreadSafe: false);Or:// Add:// using System.Threading;var lazyString1 = new Lazy&lt;string&gt;(() =&gt; \"abc\", LazyThreadSafetyMode.None);var lazyString2 = new Lazy&lt;string&gt;(() =&gt; \"abc\", LazyThreadSafetyMode.PublicationOnly);LazyThreadSafetyModeDifferent LazyThreadSafetyMode values have characteristics you may be interested in: LazyThreadSafetyMode.ExecutionAndPublication can deadlock if the initialization code has locks internally. It is also the default value. LazyThreadSafetyMode.None can help for high performance scenarios when there’s a guarantee it won’t be called simultaneously by multiple threads. LazyThreadSafetyMode.Publication allows multiple executions of the initialization code by multiple threads. The first thread to complete the initialization code sets the value.ExceptionsIn some cases an unhandled exception in the lazy initialization code will also cache returning an exception. See https://learn.microsoft.com/en-us/dotnet/framework/performance/lazy-initialization#exceptions-in-lazy-objects which contains a table describing when this occurs.Additional infoIf you’re looking to explore further try these docs pages: https://learn.microsoft.com/en-us/dotnet/api/system.lazy-1 https://learn.microsoft.com/en-us/dotnet/api/system.threading.lazythreadsafetymode https://learn.microsoft.com/en-us/dotnet/framework/performance/lazy-initialization" }, { "title": "Troubleshooting Private NuGet Packages With dotnet CLI", "url": "/troubleshooting-private-nuget-packages-with-dotnet-cli/", "categories": "", "tags": "dotnet, dotnetcore", "date": "2020-11-10 00:00:00 -0500", "snippet": ".NET now has a dotnet command line interface (CLI). Commands like dotnet build and dotnet test can be ran directly from the command line.If you’re working with private NuGet packages, that is packages not available on the public nuget.org registry, you may encounter some challenges using the dotnet CLI.Here’s a few items you may find helpful:Move nuget.config (if inside .nuget folder)Private package sources may exist in the nuget.config file. If this file is located at .nuget/nuget.config (that is, inside a .nuget folder), the dotnet CLI may have difficulty finding and using it. Consequently, the dotnet CLI might be unable to locate a package.If you’re having difficulty try moving the nuget.config file up a folder out of the .nuget folder.Provide credentials (if needed)In addition to the dotnet CLI knowing about the private NUGet package sources the dotnet CLI may need credentials to access the private package registry.If you’re working with Azure DevOps the Azure Artifacts Credential Provider is helpful. If you’re using GitHub Packages try the Configuring dotnet CLI for use with GitHub Packages documentation. If you’re using another package registry try searching for a solution.I hope this helps resolve any issues you’re having, let me know if there’s anything I’ve missed!" }, { "title": "Keyboard Shortcuts For Efficient Visual Studio 2019 Testing", "url": "/keyboard-shortcuts-for-efficient-visual-studio-2019-testing/", "categories": "", "tags": "visual-studio", "date": "2020-11-03 00:00:00 -0500", "snippet": "When authoring tests using Visual Studio 2019 you may find yourself running tests continually. Whether you’re running tests on a fresh clone to make sure tooling is working or running them while making changes to code, you may find yourself running tests frequently.You may find the experience of clicking around the UI taxing. Keyboard shortcuts can improve the experience, especially after you’ve committed them to memory and they just happen.Testing Related Keyboard ShortcutsNote: Some of the following are chords. For example, after pressing Ctrl+r you’ll see “(Ctrl+R) was pressed. Waiting for second key of chord…” on the status bar at the bottom (unless you hid the status bar, of course). Ctrl+e t: Open Test Explorer Ctrl+r a: Run all tests Ctrl+r t: Run test or tests in context of cursor Ctrl+r Ctrl+t: Debug test or tests in context of cursorOther Helpful Keyboard ShortcutsWhile not specifically testing related, here’s some others you may find helpful: Ctrl+Shift+b: Build Solution F5: Start Debugging Shift+F5: Stop Debugging Ctrl+F5: Start Without Debugging Ctrl+Alt+l: Open Solution Explorer" }, { "title": "Leniently Parse Comma Separated Configuration Values Using C#", "url": "/leniently-parse-comma-separated-configuration-values-using-csharp/", "categories": "", "tags": "dotnet, dotnetcore", "date": "2020-10-29 00:00:00 -0400", "snippet": "Whether you’re running .NET full framework or .NET Core sometimes you may want to read configuration values at runtime. Sometimes the value might look like \"a,b,c\", which some might write as \"a, b, c\". When configuration values are comma separated or delimited in some manner it’s a good idea to be forgiving when reading the input.var values = stringValue ?.Split(new[] { \",\" }, StringSplitOptions.RemoveEmptyEntries) .Select(x =&gt; x?.Trim()) .Where(x =&gt; !string.IsNullOrWhiteSpace(x)) ?? new List&lt;string&gt;();Or, as extension methods:public static class StringExtensions{ public static IEnumerable&lt;string&gt; LenientlyParseDelimited(this string str) { return LenientlyParseDelimited(str, new[] { \",\" }); } public static IEnumerable&lt;string&gt; LenientlyParseDelimited(this string str, params string[] delimiters) { if (delimiters == null) throw new ArgumentNullException(nameof(delimiters)); return str ?.Split(delimiters, StringSplitOptions.RemoveEmptyEntries) .Select(x =&gt; x?.Trim()) .Where(x =&gt; !string.IsNullOrWhiteSpace(x)) ?? new List&lt;string&gt;(); }}This approach parses the input leniently by removing empty and whitespace only entries while trimming whitespace on each split value. This can be helpful since an unexpected space character could cause an issue. You could even add additional delimiters like ';' to the array.Configuration problems can be a frustrating source of deployment issues. Leninently parsing delimited strings might just save you from a production issue." }, { "title": "Using BenchmarkDotNet with Azure Functions", "url": "/using-benchmarkdotnet-with-azure-functions/", "categories": "", "tags": "dotnetcore", "date": "2020-10-20 00:00:00 -0400", "snippet": "BenchmarkDotNet is a tool for benchmarking .NET applications. Azure Functions is a serverless web application offering. You can put these together to benchmark your Azure Functions applications.SetupUltimately, it looks like this:using BenchmarkDotNet.Attributes;using System;using System.Diagnostics;using System.Net.Http;using System.Threading.Tasks;namespace FunctionAppBenchmark{ [InProcess] public class Benchmarks { private const int FunctionsPort = 7071; private Process funcProcess; private HttpClient httpClient; [GlobalSetup] public void GlobalSetup() { funcProcess = new Process { StartInfo = { FileName = \"func.exe\", Arguments = $\"start -p {FunctionsPort}\", WorkingDirectory = \"src/FunctionApp\" } }; var started = funcProcess.Start(); if (!started) { throw new Exception(\"func.exe was not started.\"); } httpClient = new HttpClient { BaseAddress = new Uri($\"http://localhost:{FunctionsPort}\") }; } [GlobalCleanup] public void GlobalCleanup() { if (!funcProcess.HasExited) { funcProcess.Kill(); } } [Benchmark] public async Task FastEndpoint() { var response = await httpClient.GetAsync(\"/api/run\"); response.EnsureSuccessStatusCode(); } [Benchmark] public async Task SlowEndpoint() { var response = await httpClient.GetAsync(\"/api/run-slow\"); response.EnsureSuccessStatusCode(); } }}Run the benchmarkBuild and run the Release build:&gt; dotnet build -c Release&gt; dotnet .\\tests\\FunctionAppBenchmark\\bin\\Release\\netcoreapp3.1\\FunctionAppBenchmark.dllwhich produces the following:BenchmarkDotNet=v0.12.1, OS=Windows 10.0.19042Intel Core i7-1065G7 CPU 1.30GHz, 1 CPU, 8 logical and 4 physical cores.NET Core SDK=3.1.403 [Host] : .NET Core 3.1.9 (CoreCLR 4.700.20.47201, CoreFX 4.700.20.47203), X64 RyuJITJob=InProcess Toolchain=InProcessEmitToolchain Method Mean Error StdDev FastEndpoint 2.988 ms 0.1073 ms 0.3044 ms SlowEndpoint 2,030.517 ms 10.8453 ms 10.1447 ms A complete sample is available at https://github.com/kendaleiv/AzureFunctionsCSharpBenchmarkDotNet. Happy benchmarking!" }, { "title": "Peas&Carrots: The Importance Of Url Encoding Query String Parameters", "url": "/peas-and-carrots-the-importance-of-url-encoding-query-string-parameters/", "categories": "", "tags": "web", "date": "2020-05-26 00:00:00 -0400", "snippet": "When creating web applications it’s common to create links. Sometimes, links contain query string parameters. Google uses the ?q= query string parameter for search queries. An issue can arise when query string values contain characters that have special meanings.For instance, the &amp; character separates different query string items, while = separates keys and values like ?q=peas. If these characters are part of query string values, like ?q=peas&amp;carrots, carrots can be interpreted as the name of the next item in the query string, rather than part of the value of q.Here’s the problem demonstrated:new URLSearchParams('https://example.com?q=peas&amp;carrots') .has('carrots'); // true, but we want this to be falseResolutionYou can make the characters safe for a url by url encoding the query string values (as well as the keys, if necessary). With JavaScript we can do this as follows:`https://example.com?q=${encodeURIComponent('peas&amp;carrots')}`; // \"https://example.com?q=peas%26carrots\"new URLSearchParams(`https://example.com?q=${encodeURIComponent('peas&amp;carrots')}`) .has('carrots'); // false, which is what we wantThe &amp; was transformed to %26, making it safe for a url.For non-JavaScript cases follow the guidance of your specific use case. For example, with ASP.NET (both .NET Framework and .NET Core) you can use WebUtility.UrlEncode(string).NoteSome frameworks and libraries may do this automatically. But, if it’s not happening automatically it should be handled manually. If you’re not sure, try it out with peas&amp;carrots!" }, { "title": "Coding To Support Both .NET Full Framework and .NET Core", "url": "/coding-to-support-both-dotnet-full-framework-and-dotnet-core/", "categories": "", "tags": "dotnet, dotnetcore", "date": "2020-02-19 00:00:00 -0500", "snippet": "Sometimes when writing .NET libraries we want to support .NET Full Framework, as well as .NET Core. In many cases the functionality is the same, but sometimes it is different.For the cases where functionality needs to be different there’s two main concerns:Different code pathsIn many cases the code between .NET Full Framework and .NET Core can be the same for both target frameworks. But, for the cases it needs to be different we can use #if / #endif preprocessor directives to specify when code should be used.using System;using Dependency.For.Both.Target.Frameworks;#if NETSTANDARD2_0using Dependency.For.NETSTANDARD2_0.Only;#endif#if NET462using Dependency.For.NET462.Only;#endifnamespace MyLibrary{ public class MyClass { public void Run() {#if NETSTANDARD2_0 Console.WriteLine(\"netstandard2.0 only\");#endif#if NET462 Console.WriteLine(\".NET Full Framework only\");#endif Console.WriteLine(\"Both target frameworks\"); } }}You can get as creative with this technique as you’d like, including constructor parameters. You can wrap entire files with #if NET462 / #endif even!Different dependencies (NuGet packages)Sometimes you may need different NuGet packages to handle a specific target framework only. We can use a Condition to scope PackageReference items to specific target frameworks.See example MyPackage.csproj:&lt;Project Sdk=\"Microsoft.NET.Sdk\"&gt; &lt;PropertyGroup&gt; &lt;TargetFrameworks&gt;netstandard2.0;net462&lt;/TargetFrameworks&gt; &lt;/PropertyGroup&gt; &lt;ItemGroup&gt; &lt;PackageReference Include=\"Something.For.Both.Full.Framework.And.DotnetCore\" Version=\"1.0.0\" /&gt; &lt;/ItemGroup&gt; &lt;ItemGroup Condition=\"'$(TargetFramework)' == 'netstandard2.0'\"&gt; &lt;PackageReference Include=\"Something.For.DotNetCore.Only\" Version=\"1.0.0\" /&gt; &lt;/ItemGroup&gt; &lt;ItemGroup Condition=\"'$(TargetFramework)' == 'net462'\"&gt; &lt;PackageReference Include=\"Something.For.Full.Framework.Only\" Version=\"1.0.0\" /&gt; &lt;/ItemGroup&gt;&lt;/Project&gt;SummaryWriting code to support .NET Full Framework and .NET Core can bring about some extra work, but in cases where you need or want to support both this can help!" }, { "title": "Quickstart for Creating a Library Using the .NET Core CLI", "url": "/quickstart-for-creating-a-library-using-the-dotnet-core-cli/", "categories": "", "tags": "dotnet, dotnetcore", "date": "2020-02-02 00:00:00 -0500", "snippet": "The .NET Core CLI enables quickly scaffolding a new library. Here’s a quickstart of commands to use, substituting My.Sample.Project for your own new project:&gt; dotnet new --update-apply&gt; mkdir My.Sample.Project&gt; cd My.Sample.Project&gt; dotnet new sln&gt; mkdir src&gt; cd src&gt; mkdir My.Sample.Project&gt; cd My.Sample.Project&gt; dotnet new classlib&gt; cd ../..&gt; mkdir tests&gt; cd tests&gt; mkdir My.Sample.Project.Tests&gt; cd My.Sample.Project.Tests&gt; dotnet new xunit&gt; dotnet add reference ../../src/My.Sample.Project/My.Sample.Project.csproj&gt; cd ../..&gt; dotnet sln add ./src/My.Sample.Project/My.Sample.Project.csproj&gt; dotnet sln add ./tests/My.Sample.Project.Tests/My.Sample.Project.Tests.csprojIt’s all setup with a test project to begin your new library!Consider adding a build script for continuous integration, .gitignore, .editorconfig, LICENSE, and a README.md too!" }, { "title": "Using SQL Server LocalDB with Travis CI Windows Builds", "url": "/using-sql-server-localdb-with-travis-ci-windows-builds/", "categories": "", "tags": "travis-ci, continuous-integration", "date": "2019-01-05 00:00:00 -0500", "snippet": "Travis CI now includes support for Windows builds. It doesn’t have SQL Server LocalDB installed by default currently, but it’s easy to install. Here’s an example Travis CI configuration that installs it using Chocolatey:.travis.ymllanguage: shellos: windowsbefore_script:# Use mssqlserver2014-sqllocaldb rather than sqllocaldb package which has an issue:# https://dba.stackexchange.com/questions/191393/localdb-v14-creates-wrong-path-for-mdf-files- choco install mssqlserver2014-sqllocaldb- powershell -Command \"Set-ExecutionPolicy Bypass\"script:- powershell -File build.ps1Let me know if this was helpful!" }, { "title": "Using JavaScript Promises And async/await Together", "url": "/using-javascript-promises-and-async-await-together/", "categories": "", "tags": "javascript", "date": "2017-09-16 00:00:00 -0400", "snippet": "async / await in JavaScript are syntactic sugar for Promises. Since it’s syntactic sugar, they can be used interchangeably!Here’s an example:class TestClass { getPromiseValue() { return Promise.resolve('test-promise-value'); } async getAsyncValue() { return 'test-async-value'; }}//// Promise -&gt; Promise//new TestClass().getPromiseValue().then(x =&gt; { console.log(`then: ${x}`);});//// Promise -&gt; `async`//new TestClass().getAsyncValue().then(x =&gt; { console.log(`then: ${x}`);});//// `await` -&gt; Promise//async function awaitAPromise() { const testClass = new TestClass(); console.log('await: ' + await testClass.getPromiseValue());}awaitAPromise();//// `await` -&gt; `async`//async function awaitAnAsync() { const testClass = new TestClass(); console.log('await: ' + await testClass.getAsyncValue());}awaitAnAsync();Console outputthen: test-promise-valuethen: test-async-valueawait: test-promise-valueawait: test-async-valueNoteawait is only valid inside an async function!" }, { "title": "A Minimal UI Visual Studio Code Configuration", "url": "/a-minimal-ui-visual-studio-code-configuration/", "categories": "", "tags": "vscode", "date": "2017-07-23 00:00:00 -0400", "snippet": "How would you like to go from this:to this:Add the following to your settings, accessible by File → Preferences → Settings.{ \"editor.codeLens\": false, \"editor.minimap.enabled\": false, \"editor.renderIndentGuides\": false, \"window.menuBarVisibility\": \"toggle\", \"workbench.activityBar.visible\": false, \"workbench.colorTheme\": \"Default Light+\", \"workbench.iconTheme\": null, \"workbench.startupEditor\": \"newUntitledFile\", \"workbench.statusBar.visible\": false}With the activity bar hidden, toggle Explorer using Ctrl + Shift + E and Search using Ctrl + Shift + F.Press Alt to reveal the menu bar.Note that this specifies the Default Light+ color theme, which you can omit or change.Let me know in the comments if you have any suggestions for improvements!Note: This is targeting Visual Studio Code version 1.14.2." }, { "title": "A Complete Protractor Travis CI Solution for Angular E2E Testing", "url": "/a-complete-protractor-travis-ci-solution-for-angular-e2e-testing/", "categories": "", "tags": "angular", "date": "2017-04-17 00:00:00 -0400", "snippet": "TL;DR: Example at https://github.com/kendaleiv/angular-testing.Angular uses Protractor for end-to-end (e2e) testing. Protractor performs browser automation tasks using a running copy of your Angular application.Configure the projectWith continuous integration Angular builds, you generally want to run the linter, tests, and e2e tests (if you have these items).Note: If npm test is setup as a continuous test runner, you’ll want to create a test:once script or similar. If you’re using the Angular CLI, you could use something like \"test:once\": \"ng test --watch=false\".Next, assuming npm run lint and npm run test:once are setup, add \"ci\": \"npm run lint &amp;&amp; npm run test:once &amp;&amp; npm run e2e\" to your package.json scripts. You probably want to run it with npm run ci at this point to make sure it works.Add Travis CI configurationAdd the following .travis.yml file to your repository root (this expects Google Chrome to be used for e2e tests).sudo: requireddist: trustyaddons: apt: sources: - google-chrome packages: - google-chrome-stablelanguage: node_jsnode_js: - 6 - 7before_script: - export DISPLAY=:99.0 - sh -e /etc/init.d/xvfb startinstall: - npm installscript: - npm run ciNow, try it out!ExampleFor an example check out https://github.com/kendaleiv/angular-testing, which includes regular unit testing as well." }, { "title": "Providing Outdated Browser Notifications Using Angular", "url": "/providing-outdated-browser-notifications-using-angular/", "categories": "", "tags": "angular", "date": "2017-04-08 00:00:00 -0400", "snippet": "Browser-Update.org is useful for notifying users about an outdated web browser. In the case of SPA applications, the experience using an older browser can range from great to completely broken.If you’re using Angular (version 2 and later), you may be searching for an Angular specific solution for this. However, a JavaScript error while loading an Angular application due to an unsupported browser could stop the outdated browser notification from running!RecommendationInclude the JavaScript snippet from Browser-Update.org immediately before the &lt;/body&gt; closing tag, either inline or in a .js file (Content Security Policy (CSP) requirements may require using an external file). When using the Angular CLI, the Browser-Update.org script will run immediately before the Angular application’s JavaScript.Note: You may want to adjust the notifications settings for the Browser-Update.org JavaScript snippet." }, { "title": "Resolving Blacklisted RxJS Import TSLint Error", "url": "/resolving-blacklisted-rxjs-import-tslint-error/", "categories": "", "tags": "", "date": "2017-02-14 00:00:00 -0500", "snippet": "When working with Angular and the Angular CLI, one might import an RxJS Observable as:import { Observable } from 'rxjs';Later, running TSLint reports This import is blacklisted, import a submodule instead.Quick fixTo fix the error reported by TSLint, simply changeimport { Observable } from 'rxjs';toimport { Observable } from 'rxjs/Observable';Now, run TSLint and the error should be gone! However, this change may have created compilation error(s), potentially requiring importing some operators.Import operators (as necessary)After making this change, some operators may need to be imported. Here’s how to import map:import 'rxjs/add/operator/map';ExtraThis blacklisted import can be found in tslint.json as:{ \"rules\": { \"import-blacklist\": [true, \"rxjs\"] }}" }, { "title": "TypeScript Constructor Assignment: public and private Keywords", "url": "/typescript-constructor-assignment-public-and-private-keywords/", "categories": "", "tags": "typescript", "date": "2017-02-04 00:00:00 -0500", "snippet": "TypeScript includes a concise way to create and assign a class instance property from a constructor parameter.Rather than:class TestClass { private name: string; constructor(name: string) { this.name = name; }}One can use the private keyword instead:class TestClass { constructor(private name: string) { }}The public keyword works in the same fashion, but also instructs the TypeScript compiler that it’s OK to access the property from outside the class.Here’s a more complete example including the public keyword, as well as the result of not including a keyword:class TestClass { constructor(name: string, private address: string, public city) { } testMethod() { console.log(this.name) // Compiler error: Property 'name' does not exist on type 'TestClass'. console.log(this.address); console.log(this.city); }}const testClass = new TestClass('Jane Doe', '123 Main St.', 'Cityville');testClass.testMethod();console.log(testClass.name); // Compiler error: Property 'name' does not exist on type 'TestClass'.console.log(testClass.address); // Compiler error: 'address' is private and only accessible within class 'TestClass'.console.log(testClass.city);Love this shortcut? Not a fan? Let me know in the comments!" }, { "title": "Run Custom Functionality When Angular Http Errors Occur", "url": "/run-custom-functionality-when-angular-http-errors-occur/", "categories": "", "tags": "angular", "date": "2017-01-31 00:00:00 -0500", "snippet": "Angular includes its own functionality for making HTTP requests. When using it, you may want to react to any errors that happen – perhaps to display an on-screen error message.You can achieve that by extending Http.custom-http.service.tsimport { Injectable } from '@angular/core';import { ConnectionBackend, Http, Request, RequestOptions, RequestOptionsArgs, Response } from '@angular/http';import { Observable } from 'rxjs/Observable';import 'rxjs/add/observable/throw';import 'rxjs/add/operator/catch';@Injectable()export class CustomHttpService extends Http { constructor(backend: ConnectionBackend, options: RequestOptions) { super(backend, options); } request(url: string | Request, options?: RequestOptionsArgs): Observable&lt;Response&gt; { return super .request(url, options) .catch(err =&gt; { console.log(err); // Run any custom functionality here! return Observable.throw(err); }); }}Now, use it in an NgModule:app.module.tsimport { BrowserModule } from '@angular/platform-browser';import { NgModule } from '@angular/core';import { Http, HttpModule, RequestOptions, XHRBackend } from '@angular/http';import { AppComponent } from './app.component';import { CustomHttpService } from './custom-http.service';export function customHttpFactory(backend: XHRBackend, options: RequestOptions) { return new CustomHttpService(backend, options);}@NgModule({ declarations: [ AppComponent ], imports: [ BrowserModule, HttpModule ], providers: [ { provide: Http, useFactory: customHttpFactory, deps: [XHRBackend, RequestOptions] } ], bootstrap: [AppComponent]})export class AppModule { }Let me know in the comments what you build with this!" }, { "title": "Comparing Angular NgModule and TestBed Side by Side", "url": "/comparing-angular-ngmodule-and-testbed-side-by-side/", "categories": "", "tags": "angular", "date": "2017-01-17 00:00:00 -0500", "snippet": "TestBed’s configureTestingModule method looks similar to NgModule in usage. Check out this side by side example:@NgModule({ | TestBed.configureTestingModule({ | declarations:[ | declarations: [ AppComponent, | AppComponent, CurrentResultsComponent, | CurrentResultsComponent, ResultsLogComponent | ResultsLogComponent ], | ], imports: [ | imports: [ BrowserModule, | FormsModule, FormsModule, | HttpModule HttpModule | ], | ], providers: [ | providers: [ StockRetrieverService | { ], | provide: StockRetrieverService, | useClass: TestStockRetrieverService | } | ] | | bootstrap: [AppComponent] | }) | export class AppModule { } | });For a full example see the https://github.com/kendaleiv/angular-testing repository." }, { "title": "2016: Personal Highlights", "url": "/2016-personal-highlights/", "categories": "", "tags": "2016, highlights", "date": "2016-12-31 00:00:00 -0500", "snippet": "I posted a personal recap last year, and the year before that. For this year, I’m calling it “2016: Personal Highlights”: I received my first Microsoft MVP award in Visual Studio and Development Technologies. It’s awesome to be recognized for my efforts! I spoke at my first international conference this year: NDC Oslo. It was amazing to be selected for such a great opportunity! Looking forward to continuing to speak at conferences and events in the future. And, if you’re interested, you can watch the talk I gave on Vimeo: We Replaced a Multi-Application Home-Grown Authentication System." }, { "title": "Subscribing to Browser Title Changes Using Angular", "url": "/subscribing-to-browser-title-changes-using-angular/", "categories": "", "tags": "angular", "date": "2016-12-26 00:00:00 -0500", "snippet": "Angular includes a Title service, which includes getTitle(): string and setTitle(newTitle: string). If you want to keep an in-page header matching the browser title tag, you’ll need something to enable that possibility.If you like the pattern of subscribing to changes (as you might be, if you’re using Angular), check out the following SubscribableTitleService://// src/app/subscribable-title.service.ts//import { Injectable } from '@angular/core';import { Title } from '@angular/platform-browser';import { BehaviorSubject } from 'rxjs/BehaviorSubject';@Injectable()export class SubscribableTitleService { public title: BehaviorSubject&lt;string&gt;; constructor(private titleService: Title) { this.title = new BehaviorSubject&lt;string&gt;(titleService.getTitle()); } setTitle(newTitle: string) { this.titleService.setTitle(newTitle); this.title.next(newTitle); }}Now, to use it in a page header://// src/app/header/header.component.ts//import { Component, OnInit } from '@angular/core';import { SubscribableTitleService } from '../subscribable-title.service';@Component({ selector: 'app-header', templateUrl: './header.component.html', styleUrls: ['./header.component.scss']})export class HeaderComponent implements OnInit { public headerText: string; constructor(private subscribableTitleService: SubscribableTitleService) { } ngOnInit() { this.subscribableTitleService.title.subscribe(title =&gt; { this.headerText = title; }); }}With the following or similar src/app/header/header.component.html file:&lt;h1&gt;{{ headerText }}&lt;/h1&gt;CreditCredit to https://hassantariqblog.wordpress.com/2016/12/03/angular2-set-page-title-dynamically-as-angular-service-in-angular-2-application/ for a similar strategy, but this blog post’s SubscribableTitleService adds functionality for manipulating the browser’s title tag." }, { "title": "Angular 2 Services Testing Template", "url": "/angular-2-services-testing-template/", "categories": "", "tags": "angular, testing", "date": "2016-10-23 00:00:00 -0400", "snippet": "Here’s a simple template for testing Angular 2 services.If the HttpModule portions are uncommented and used, it will make actual wire calls. To use mock calls instead, see Angular 2 MockBackend Service Testing Template Using TestBed instead.import { async, inject, TestBed } from '@angular/core/testing';// import { HttpModule } from '@angular/http';import { SomeService } from './some.service';describe('SomeService', () =&gt; { beforeEach(() =&gt; { TestBed.configureTestingModule({ providers: [ SomeService ], imports: [ // HttpModule ] }); }); it('should construct', async(inject([SomeService], (service) =&gt; { expect(service).toBeDefined(); })));});Also, if you’re looking for a component testing template, see Angular 2 Component Testing Template Using TestBed." }, { "title": "Angular 2 MockBackend Service Testing Template Using TestBed", "url": "/angular-2-mockbackend-service-testing-template-using-testbed/", "categories": "", "tags": "angular, testing", "date": "2016-10-23 00:00:00 -0400", "snippet": "Below is a template for using TestBed and MockBackend for mocking Angular 2 HTTP calls.If you want to make actual wire calls, see Angular 2 Services Testing Template instead.import { async, inject, TestBed } from '@angular/core/testing';import { BaseRequestOptions, Http, HttpModule, Response, ResponseOptions } from '@angular/http';import { MockBackend } from '@angular/http/testing';import { SomeService } from './some.service';describe('SomeService (Mocked)', () =&gt; { beforeEach(() =&gt; { TestBed.configureTestingModule({ providers: [ SomeService, MockBackend, BaseRequestOptions, { provide: Http, useFactory: (backend, options) =&gt; new Http(backend, options), deps: [MockBackend, BaseRequestOptions] } ], imports: [ HttpModule ] }); }); it('should construct', async(inject( [SomeService, MockBackend], (service, mockBackend) =&gt; { expect(service).toBeDefined(); }))); describe('someMethod', () =&gt; { const mockResponse = { color: 'blue' }; it('should parse response', async(inject( [SomeService, MockBackend], (service, mockBackend) =&gt; { mockBackend.connections.subscribe(conn =&gt; { conn.mockRespond(new Response(new ResponseOptions({ body: JSON.stringify(mockResponse) }))); }); const result = service.someMethod(); result.subscribe(res =&gt; { expect(res).toEqual({ color: 'blue' }); }); }))); });});For a working example see https://github.com/kendaleiv/angular-testing/blob/master/src/app/stock-retriever.service.spec.ts.–If you’re interested, check out Angular 2 Component Testing Template Using TestBed." }, { "title": "Angular 2 Component Testing Template Using TestBed", "url": "/angular-2-component-testing-template-using-testbed/", "categories": "", "tags": "angular, testing", "date": "2016-10-18 00:00:00 -0400", "snippet": "Below is a template for using TestBed to test Angular 2 components. Let me know in the comments if I’ve missed something noteworthy!import { async, TestBed } from '@angular/core/testing';import { SomeComponent } from './some.component';beforeEach(() =&gt; { TestBed.configureTestingModule({ declarations: [ SomeComponent ], imports: [ // HttpModule, etc. ], providers: [ // { provide: ServiceA, useClass: TestServiceA } ] });});it('should do something', async(() =&gt; { // Overrides here, if you need them TestBed.overrideComponent(SomeComponent, { set: { template: '&lt;div&gt;Overridden template here&lt;/div&gt;' // ... } }); TestBed.compileComponents().then(() =&gt; { const fixture = TestBed.createComponent(SomeComponent); // Access the dependency injected component instance const app = fixture.componentInstance; expect(app.something).toBe('something'); // Access the element const element = fixture.nativeElement; // Detect changes as necessary fixture.detectChanges(); expect(element.textContent).toContain('something'); });}));For a template showing MockBackend usage while testing a service with TestBed, see Angular 2 MockBackend Service Testing Template Using TestBed." }, { "title": "100% Code Coverage Doesn't Mean It Works", "url": "/100-percent-coverage-doesnt-mean-it-works/", "categories": "", "tags": "tests, coverage", "date": "2016-07-07 00:00:00 -0400", "snippet": "Tests are great. I like testing, tests, and talking about testing and tests.I think code coverage is useful. It can identify gaps in testing, providing a “hey, maybe you should test over here” notion. But, 100% coverage doesn’t mean everything works perfectly. 100% test coverage doesn’t mean anything works properly. It means all the code was executed in some fashion. All statements were visited. Basically, the only thing it shows is the code didn’t unexpectedly throw an unhandled error.No assertions, 100% coverageHere’s an example of 100% code coverage with no asserts:function isNumberSeven(number) { return number === 7;}// Jasminedescribe('isNumberSeven tests', () =&gt; { it('runs', () =&gt; { isNumberSeven(); });});Todd Gardner refers to this as “Assertion-Free Testing” in his Software Testing for Failed Projects talk at NDC Oslo 2016.Assertions! And, more tests!Using assertions (and, more tests) dramatically improves the quality of the test suite.// Jasminedescribe('isNumberSeven tests', () =&gt; { it('returns true for 7', () =&gt; { expect(isNumberSeven(7)).toBe(true); }); it('returns false for 6', () =&gt; { expect(isNumberSeven(6)).toBe(false); }); it('returns false for 8', () =&gt; { expect(isNumberSeven(8)).toBe(false); });});Use code coverage, not because you “have to”Code coverage can be a useful tool to show trends and identify potential gaps in testing. But, using it as a blind metric could be harmful. If you’re mandated to abide by a certain percentage of coverage, you can technically achieve the goal without getting the value you’d expect out of good, well-authored tests." }, { "title": "Comparing Classes: C# 6, ES6/ES2015 JavaScript, and TypeScript", "url": "/comparing-classes-csharp6-es6-es2015-javascript-and-typescript/", "categories": "", "tags": "csharp, javascript, typescript", "date": "2016-07-05 00:00:00 -0400", "snippet": "Classes are a useful construct typically associated with object-oriented programming.In C# classes are a first-class construct of the language. Classes were introduced a bit later for JavaScript. Classes in JavaScript are simply syntactic sugar.C# 6public class Person{ private readonly SomeDependency _someDependency; public Person(SomeDependency someDependency) { _someDependency = someDependency; } public string GivenName { get; set; } public string FamilyName { get; set; } public string Name =&gt; $\"{GivenName} {FamilyName}\";}public class SomeDependency { }// Usage:var person = new Person(new SomeDependency());person.GivenName = \"Ken\";person.FamilyName = \"Dale\";Console.WriteLine(person.Name); // Ken DaleJavaScript (ES6 / ES2015)class Person { constructor(someDependency) { this.someDependency = someDependency; } get name() { return `${this.givenName} ${this.familyName}`; }}class SomeDependency {}// Usage:const person = new Person(new SomeDependency());person.givenName = \"Ken\";person.familyName = \"Dale\";console.log(person.name); // Ken DaleTypeScript v1class Person { constructor(public someDependency: SomeDependency) { } givenName: string; familyName: string; get name(): string { return `${this.givenName} ${this.familyName}`; };}class SomeDependency { }// Usage:const person = new Person(new SomeDependency());person.givenName = \"Ken\";person.familyName = \"Dale\";console.log(person.name); // Ken DaleI hope this brings clarity by comparing these different languages." }, { "title": "We Replaced a Multi-Application Home-Grown Authentication System", "url": "/we-replaced-a-multi-application-home-grown-authentication-system/", "categories": "", "tags": "authentication", "date": "2016-06-27 00:00:00 -0400", "snippet": "On June 9, 2016 I had an amazing opportunity to deliver a talk bearing the name of this post title at NDC Oslo. An embed of the video is at the end of this post.Here’s a write-up:Where We StartedThe existing implementation had two primary tenets: Centralized authentication Per-application authorizationThe prior solution was built on top of ASP.NET Forms Authentication, but enhanced to include centralized authentication, impersonation functionality, and a basis for per-application authorization.We kept these primary tenets intact with the replacement rollout.Our rationaleOur main goals included supporting new scenarios and using a modern specification.Our desired immediate and future login possibilities included Active Directory logins, social logins, as well as a login story for SPA and other applications.In terms of following a specification, we chose OpenID Connect.Exploring choicesWe explored different choices for implementing our desired feature set. It ultimately boiled down to choosing from the following: Write it yourself (not an actual solution for many developers and teams…) Find a good and reliable open source implementation Use an online serviceWe chose to cloud host an open source implementation using IdentityServer3 and BrockAllen.MembershipReboot.ImplementationNow that a decision has been made, it’s time to get moving. I’ll caution you: It’s going to look easy. Diagrams like this one can lure you into a false sense of you-think-you-know-what-you’re-doing:Diagram: https://identityserver.github.io/Documentation/assets/images/terminology.png on https://identityserver.github.io/Documentation/docsv2/overview/terminology.htmlNow, start thinking and implementing. You may realize you don’t know what to do, or even where to begin. If that’s the case – it’s totally OK. These diagrams are great for a high-level understanding, but you’ll need a much deeper understanding to be able to implement this.As for us, we got expert help: An on-site consultant for one week to rocket the project forward quickly. This was highly valuable for our team.Making it easy: Abstraction(s)You’ll likely want to create an abstraction or abstractions for use in applications as a standard way to wire up the authentication/authorization solution. A consistent, easy to use abstraction can be useful!You choose the abstraction(s)! And, how to distribute them.An example abstraction could look something like this:[assembly: OwinStartup(typeof(TheApp.Startup))]public class Startup{ public void Configuration(IAppBuilder app) { var config = TheOrgConfiguration.GetFromAppSettings(); app.UseStandardTheOrgAuthWithRoles(config, currentUser =&gt; { // Return roles for `currentUser` }); }}Unfriendly by designAuthentication and authorization is unfriendly by design. No hints are given if 15 characters of a 16 character password or correct. It’s just nope. Having logging in place to help you as a developer or sysadmin discover the real underlying problems (scope problems, etc.) could go a long way.Configuration, configuration, configurationConfiguration plays a strong role in an authentication and authorization solution working as expected or not. A simple misconfiguration can cause huge problems.When it comes to configuration: Prefer less to more. Combine and simplify configuration where possible. If there’s a configuration setting you never expect to change, consider hardcoding it. Even if you do change it in the future – it’s a quick deployment. Not having to deal with it over time may be an overall win.Limit visiblity of secrets to those who should be able to access them. If a team member doesn’t have full production environment access, they shouldn’t be able to get a production environment secret, either.LoggingLogging can be helpful if things don’t go as planned. If there’s an issue in production, having more information at hand may help you discover the root problem more quickly. Ideally, any logging strategy makes it easy to find what you’re looking for, without having to sift through a bunch of non-applicable items.If you’re logging unhandled exceptions or other events, consider filtering or removing items that aren’t real problems. A user providing bad input and other errors-that-aren’t-necessarily-errors can dilute your error logs if left unchecked.ImpersonationI took a deep dive into impersonation in terms of demo’ing Stuntman, an open source ASP.NET library for runtime user impersonation/selection. Here’s what Stuntman can do for you in a browser:I also demo’ed usage of an access token configured for a user in Stuntman with Fiddler and an Authorization: Bearer the_access_token request header.VideoAnd, here’s the video:" }, { "title": "TypeScript: Where's My C# Style Object Initializers?", "url": "/typescript-wheres-my-csharp-style-object-initializers/", "categories": "", "tags": "typescript", "date": "2016-04-26 00:00:00 -0400", "snippet": "In C#, you’re able to initialize an object while newing it up.using System;public class Program{ public static void Main() { var person = new Person { GivenName = \"John\", FamilyName = \"Doe\" }; Console.WriteLine(person.Name); // John Doe }}public class Person{ public string GivenName { get; set; } public string FamilyName { get; set; } public string Name =&gt; $\"{GivenName} {FamilyName}\";}However, this specific syntax is not available in TypeScript. We could simply do it long form:class Person { givenName: string; familyName: string; get name(): string { return `${this.givenName} ${this.familyName}`};}const person = new Person();person.givenName = 'John';person.familyName = 'Doe';console.log(person.name); // John DoeOr, you can try something like this:// Attribution: This idea is from http://stackoverflow.com/a/14142198/941536interface Person { givenName: string; familyName: string;}const person: Person = { givenName: 'John', familyName: 'Doe'};Note that the previous code example requires that givenName and familyName must be implemented. If that’s exactly what you want, this works OK.But, if Person is a class instead and you need getters or other functionality to work, and you’re able to modify this class, it might make sense to use the constructor:// Attribution: This idea is from http://stackoverflow.com/a/14226836/941536class Person { constructor(public givenName: string, public familyName: string) { } get name(): string { return `${this.givenName} ${this.familyName}`};}const person = new Person(\"John\", \"Doe\");console.log(person.name); // John DoeAlternatively, the constructor could accept and work with an object instead, as suggested in https://github.com/Microsoft/TypeScript/issues/3895#issuecomment-122046396.If you’ve got a better idea (that’s preferably still lightweight) let me know in the comments!" }, { "title": "TypeScript: Missing Properties When Using Type Assertions", "url": "/typescript-missing-properties-when-using-type-assertions/", "categories": "", "tags": "typescript", "date": "2016-04-19 00:00:00 -0400", "snippet": "In C# and other languages, you may be familiar with the concept of a cast – basically, explicitly coercing a type into another type. In C# this looks like (MyClass)x.When using a C# cast, all of the properties and functionality of the destination type are available as expected. In C#, if there’s a property like public string SomeProperty { get { return _someProperty; } }, this getter will function as expected. Basically, if the destination type is T the post-cast object is a fully functional instance of T.However, the &lt;MyClass&gt;obj syntax of TypeScript is a type assertion, not a cast. It’s a hint as to the type, which doesn’t actually transform the underyling JavaScript object. If the underlying JavaScript object does not have a set of properties, using type assertion will not apply them in the same manner as a cast would.Consider the following TypeScript example:class Person { constructor(public givenName: string, public familyName: string) { } get name() { return `${this.givenName} ${this.familyName}`; }}const person = new Person(\"John\", \"Doe\");const personFromJson = &lt;Person&gt;JSON.parse( '{ \"givenName\": \"John\", \"familyName\": \"Doe\" }');console.log(person.name); // John Doeconsole.log(personFromJson.name); // undefinedIt’s important to recognize the difference between type assertions and casts when working with TypeScript." }, { "title": "Lindt Dark Chocolate Percentage EXCELLENCE Bars: Nutrition Comparison", "url": "/lindt-dark-chocolate-percentage-excellence-bars-nutrition-comparison/", "categories": "", "tags": "chocolate", "date": "2016-04-05 00:00:00 -0400", "snippet": "Here’s a comparison of the dark chocolate Lindt EXCELLENCE bars that have a minimum percentage of cocoa associated with them. Enjoy!(Note: bold text is mine for emphasis) Cocoa % 70% 85% 90% 99% Serving Size (in grams) 40g 40g 40g 40g Serving Per Container 2.5 2.5 2.5 about 1 Calories 250 230 240 220 - Calories from Fat 170 170 190 170 Total Fat 19g 18g 22g 20g - Saturated Fat 12g 11g 13g 12g - Trans Fat 0g 0g 0g 0g Cholesterol 0mg 0mg 0mg 0mg Sodium 10mg 15mg 10mg 15mg Total Carbohydrate 17g 15g 12g 12g - Dietary Fiber 3g 6g 5g 7g - Sugars 12g 5g 3g 1g Protein 3g 5g 4g 5g Links: Lindt 70% Cocoa EXCELLENCE Bar http://www.lindtusa.com/shop/70-cocoa-excellence-bar Lindt 85% Cocoa EXCELLENCE Bar http://www.lindtusa.com/shop/85-cocoa-excellence-bar Lindt 90% Cocoa EXCELLENCE Bar http://www.lindtusa.com/shop/90-cocoa-excellence-bar Lindt 99% Cocoa EXCELLENCE Bar http://www.lindtusa.com/shop/99-cocoa-excellence-bar-391872-pIf anything here changes in the future, let me know!" }, { "title": "Can We Make Some Changes First? Pull Requesting a GitHub Pull Request", "url": "/can-we-make-some-changes-first-pull-requesting-a-github-pull-request/", "categories": "", "tags": "github", "date": "2016-02-16 00:00:00 -0500", "snippet": "Pull request contributions are awesome. But, sometimes prior to merge there’s a few things you’d like changed – or, perhaps more than a few. As a reviewer, if you feel like your being too picky but want a number of changes made prior to merge, submitting a pull request to a pull request branch can be a nice gesture. It shows you’re willing to put forth the effort for the changes you’re desiring.HowHere’s a few commands to get you on your way. Note that REMOTE_NAME refers to the name of the new remote being added. This can be anything. You may want to make it their GitHub username, or something else.&gt; git remote add REMOTE_NAME CONTRIBUTOR_REPOSITORY_URL&gt; git fetch REMOTE_NAME&gt; git checkout -b REMOTE_NAME-the-pull-request-branch REMOTE_NAME/the-pull-request-branch&gt; git checkout -b suggestions-for-the-pull-requestMake and commit any changes, then:&gt; git push origin suggestions-for-the-pull-requestNow, create a pull request on GitHub, targeting the fork and branch the contributor made their pull request from.After the pull request is made, you may want to write a short explanation if the contributor is potentially unfamiliar with this practice.In closingIf the contributor agrees to merge your suggested changes, then everything is merged exactly as you like. And, the contributor feels valued in that you went out of your way to modify their pull request to help it meet your standards. Basically, you did some work in response to their work – demonstrating that you’re willing to put some effort in as well.Sometimes, creating a pull request targeting a pull requested branch might be just what you’re looking for to get everything fixed up perfectly and make it a positive experience for a contributor." }, { "title": "GitHub Issues: Let's Take Action", "url": "/github-issues-lets-take-action/", "categories": "", "tags": "issues", "date": "2016-01-18 00:00:00 -0500", "snippet": "As a development team, you’re probably tracking your work somehow – whether it’s GitHub issues or some other application or system. Which is great! Things are being documented, worked on, and completed.However, for various reasons, sometimes issues don’t get worked on and completed. This isn’t necessarily a bad thing – perhaps the issue just didn’t matter enough when compared to other more important issues.These unhandled issues can pile up. When issues pile up important and unimportant work is mixed together. And, after enough time passes you’re questioning if some issues are still relevant at all.Taking actionI suggest manually (or, automatically) closing issues for inactivity. If it hasn’t been important enough to be completed in a short amount of time – perhaps it’s not important enough to ever be completed.For a business environment, this is probably best done manually. You may need to constantly reevaluate the situation and be honest: If you can’t see the issue being completed, maybe it should be closed. I believe Khalid Abuhakmeh has mentioned something like if it’s important enough, it’ll come back up again aloud previously. And, it makes sense. You’ll probably continue to remember an overdue oil change until you take care of it – even if you don’t write it down or plan it out.For an open source project (especially a popular one), automatically closing issues waiting too long for the issue reporter to reply using a bot or similar may be a great course to take. I don’t have current experience here, but it seems like https://github.com/twbs/no-carrier might be something to evaluate.In closingHaving carefully pruned issues that always matter enables team members and contributors to know what’s important enough to act on. Without this discipline, it’s easy for issues to become a sea of uncertainty. Should we implement this? Should we fix this? Does this still apply? Hmm, it’s many months old…Let’s take action! Even if the action is simply deciding to not do something and closing the issue, at least we’re making a decision and moving forward. It’s easy to change your mind later and reopen an issue, if the need arises." }, { "title": "Copying App Settings and Connection Strings Between Azure Web Apps", "url": "/copying-app-settings-and-connection-strings-between-azure-web-apps/", "categories": "", "tags": "azure, configuration", "date": "2016-01-12 00:00:00 -0500", "snippet": "You have a few options if you’re looking to copy Azure app settings and connection strings between Azure Web Apps. One of those options is using the Azure portal, copy and pasting until the task is completed.Alternatively, you could use a tool to help automate the process. One such tool is the Azure Web App Configuration Copier – which was created to fill a potential need by the Ritter Insurance Marketing development team.For usage instructions, see the README.md in the ritterim/azure-web-app-configuration-copier GitHub repository.For more information, see this post on the RIMdev blog." }, { "title": "2015: A Personal Recap", "url": "/2015-a-personal-recap/", "categories": "", "tags": "2015, recap", "date": "2015-12-31 00:00:00 -0500", "snippet": "I posted a personal recap last year. Here’s the same as a 2015 edition: I spoke at my first conference this year! In fact, I spoke at 7 conferences this year, as well as some other non-conference events! It’s been an amazing experience traveling, meeting new people, and just having a blast. Thanks to everyone who inspired me to get this far! I bought a musical keyboard – a 61 key MIDI controller. And, I lead my first-ever solo (small) Worship service using it! I got some internet points for authoring the Slack Chocolatey package. You may express your gratitude each time you choco install slack to my Twitter account (not required, but, I wouldn’t complain). I tracked around 250 miles on my Runkeeper account.See you in 2016, internet!" }, { "title": "Violin Backed by Gibber", "url": "/violin-backed-by-gibber/", "categories": "", "tags": "music, violin, gibber", "date": "2015-11-29 00:00:00 -0500", "snippet": "As a developer, do you frequently find yourself longing for a backup band to your melodic instrument? If this sounds like you (or, even if it doesn’t), check this out:The Gibber code// Roland TR-808 simulation:// Kick, rest, snare, rest, kick, kick, snare, resta = XOX('x.o.xxo.')// Closed hihat, eighth notesb = Drums('********')c = FM('bass').note.seq( ['g', 'g', 'g', 'g', 'd', 'd', 'd', 'd', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'd'], 1/4)c.amp = .3d = Synth2({ maxVoices: 3, decay: 1/2 }).chord.seq( ['g3', 'd3', 'c3', 'd3'], 1)d.amp = .5In closingGive Gibber a try at http://gibber.mat.ucsb.edu/.If you’re wondering what Gibber is check out http://charlie-roberts.com/gibber/about-gibber.Note: As of the date of this blog post, Gibber doesn’t play audio for me using Chrome 46 (64-bit) on Windows 10. It does work for me using Firefox 42 (32-bit, I assume, it’s installed in my Program Files (x86) folder) on Windows 10, though." }, { "title": "Microsoft Windows: Update Chocolatey, Atom, npm, and RubyGems packages cmd Script", "url": "/microsoft-windows-update-chocolatey-atom-npm-and-rubygems-packages-cmd-script/", "categories": "", "tags": "windows, update, packages", "date": "2015-11-11 00:00:00 -0500", "snippet": "Looking for a way to update various packages in your Windows environment? Use the script below running as Administrator (Administrator is needed for Chocolatey, at least), editing as necessary for your needs.Note: This cmd script is unprompted with no opportunity for review.update-packages.cmd@echo offecho.echo ## Chocolatey packagesecho.call choco upgrade all -yecho.echo ## Atom packagesecho.call apm upgrade --no-confirmecho.echo ## npm packagesecho.call npm update -gecho.echo ## RubyGemsecho.call gem updateThis will handle updates for Chocolatey, Atom, npm, and RubyGems packages.Feel free to use and adapt this cmd script for your needs. Happy updating!" }, { "title": "IEMs and Headphones for Developer Use", "url": "/iems-and-headphones-for-developer-use/", "categories": "", "tags": "iem, headphones", "date": "2015-10-20 00:00:00 -0400", "snippet": "You may have seen this before:This is why you shouldn&#39;t interrupt a programmer: http://t.co/K2dNXKzjem&mdash; Jason Heeris (@detly) October 28, 2013Flow state is important for developers – possibly more important for some individuals than others. If nothing else, sometimes it’s nice to block out random noises of the day.Some important factors I see for developers choosing IEMs (in-ears monitors) or headphones are isolation, comfort, convenience, safety, and sound quality.IsolationIf your primary goal is blocking outside noise, you’ll want to examine the isolation capabilities of your device.Check out this graph courtesy of http://www.headphone.com/pages/build-a-graph:Look at the dramatic difference between different devices – a 3dB decrease is a halving of volume!The Etymotic hf5 is an IEM. The Sennheiser HD280 Pro and Bose QC3 are headphones, with the latter having active noise cancellation . The Apple Stock Bud New is included for comparison.This might be obvious, but too much isolation could be a problem, if you want to hear certain things (teammates chatting, alarms, etc.).ComfortIf you’re wearing something for many hours, comfort is important. Comfort is an individual thing. Some many find IEMs more comfortable then headphones, while others may loathe the idea of having something in their ear for an extended period. The mass and clamping force of headphones may be a bother to some.You’ll need to make your own determination here.ConvenienceIEMs can be more convenient from an overall size standpoint, but there may be some cleaning related maintenance associated with them. Headphones are larger, but may have less cleaning maintenance. Also, headphones may be more convenient if you need to frequently remove them.SafetyListening too loud can damage your hearing. That said, having better isolation may allow you to listen to music at a lower volume, if the goal is to block outside noise.Also, foreign bodies in your ear can also cause health problems, ranging from annoying to serious. Different people may have different tolerances to having IEMs lodged in their ears for extended periods – don’t exceed yours. Also, if you’re an IEM user, regular cleaning of your IEMs is probably a good idea.Sound qualityThis might be a lesser concern if music is background to your work. I’m not suggesting it doesn’t matter, but, it’s ideally good enough that it isn’t bothersome. If you might do some critical listening, though, keep that in mind.Bonus: Built-in microphonesIf your device microphone(s) aren’t good enough, a microphone built into a headphone cable or similar is probably a poor substitute for a better microphone. It might be useful for a smartphone scenario, but, I can’t really speak to that personally.It’s up to youYou’ll need to make a decision on what’s important for you. How much do you want to isolate yourself from the world? Does your body respond well to having something in your ear all day?Hopefully this helps you on your quest for the best set of IEMs or headphones for your needs." }, { "title": "Integrating Google Chrome With KeePass Using chromeIPass and KeePassHttp", "url": "/integrating-google-chrome-with-keepass-using-chromeipass-and-keepasshttp/", "categories": "", "tags": "chrome, keepass, chromeipass, keepasshttp", "date": "2015-10-14 00:00:00 -0400", "snippet": "Having browser integration with KeePass can be helpful, and can be accomplished with Google Chrome using KeePassHttp and chromeIPass.Here’s how:Install KeePassHttpYou can install KeePassHttp on Windows using Chocolatey andchoco install keepass-keepasshttp from an Administrator cmd or PowerShell prompt.If you had KeePass open during the installation, you’ll want to restart it to begin using the KeePassHttp plugin.If you’re running OS X or Linux, or prefer to not use Chocolatey on Windows, refer to the directions at https://github.com/pfn/keepasshttp.Install chromeIPasschromeIPass is a Google Chrome plugin designed to work with KeePassHttp. You can install it from https://chrome.google.com/webstore/detail/chromeipass/ompiailgknfdndiefoaoiligalphfdae.Quick setupAfter everything is installed, click on the icon for the chromeIPass extension in Google Chrome, then click on the Connect button. Now, complete the connection procedure in KeePass.That concludes this quick how-to, enjoy!" }, { "title": "Once Per Test Run: Using the Azure Storage Emulator with xUnit.net v2", "url": "/once-per-test-run-using-the-azure-storage-emulator-with-xunit-net-v2/", "categories": "", "tags": "csharp, xunit, azure, storage, emulator", "date": "2015-10-07 00:00:00 -0400", "snippet": "If you’re using the Azure Storage Emulator as part of a suite of integration tests using xUnit.net v2, ideally you start and stop the emulator only once per test run. With xUnit.net v2, you can accomplish this using ICollectionFixture&lt;TFixture&gt; and decorating your test classes with CollectionAttribute.By using the CollectionAttribute, you’re instructing xUnit.net 2.x to run these tests in serial (which, is good, since parallel tests in this manner could be problematic).Check this out:[CollectionDefinition(\"AzureStorageIntegrationTests\")]public class AzureStorageEmulatorCollection : ICollectionFixture&lt;AzureStorageEmulatorFixture&gt;{}public class AzureStorageEmulatorFixture : IDisposable{ private readonly AzureStorageEmulatorAutomation _automation; public AzureStorageEmulatorFixture() { _automation = new AzureStorageEmulatorAutomation(); Console.WriteLine(\"----- Invoking Automation Start -----\"); _automation.Start(); } public void Dispose() { Console.WriteLine(\"----- Invoking Automation Dispose -----\"); _automation.Dispose(); }}[Collection(\"AzureStorageIntegrationTests\")]public class IntegrationTestClass1{ [Fact] public void VerifyEmulatorIsRunning() { Assert.True(AzureStorageEmulatorAutomation.IsEmulatorRunning()); }}[Collection(\"AzureStorageIntegrationTests\")]public class IntegrationTestClass2{ [Fact] public void VerifyEmulatorIsRunning() { Assert.True(AzureStorageEmulatorAutomation.IsEmulatorRunning()); }}A full example of this is available at https://github.com/kendaleiv/azure-storage-integration-tests." }, { "title": "Starting, Stopping, and Clearing the Azure Storage Emulator in C#", "url": "/starting-stopping-and-clearing-the-azure-storage-emulator-in-csharp/", "categories": "", "tags": "csharp, integration, tests, azure, storage, emulator", "date": "2015-10-01 00:00:00 -0400", "snippet": "If you’re writing code that uses Microsoft Azure Storage blobs, tables, or queues, you can use the storage emulator on Windows for local development – rather than relying on the cloud while developing. And, using the RimDev.Automation.StorageEmulator NuGet package, you can interact with it in C#!First, install the RimDev.Automation.StorageEmulator NuGet package.PM&gt; Install-Package RimDev.Automation.StorageEmulatorNow, it’s as simple as:var automation = new AzureStorageEmulatorAutomation();automation.Start();AzureStorageEmulatorAutomation.IsEmulatorRunning(); // should be trueautomation.ClearAll();// Or, clear only certain things:automation.ClearBlobs();automation.ClearTables();automation.ClearQueues();automation.Stop();AzureStorageEmulatorAutomation.IsEmulatorRunning(); // should be falseAzureStorageEmulatorAutomation implements IDisposable, too, but it only stops the emulator on dispose if that specific automation instance is what started it:using (var automation = new AzureStorageEmulatorAutomation()){ automation.Start(); // Work with the running Azure Storage Emulator here.}// Outside the scope of the using, if the Azure Storage Emulator was// started by `automation.Start();` above, then it should be shut down.// If it was already running, it should remain running.Closing noteThe GitHub repository associated with the RimDev.Automation.StorageEmulator NuGet package is https://github.com/ritterim/automation-storage-emulator." }, { "title": "Displaying DateTime in Browser Time Zone in ASP.NET Razor Views", "url": "/displaying-datetime-in-browser-time-zone-in-aspnet-razor-views/", "categories": "", "tags": "aspnet, razor, datetime, browser, time, zone", "date": "2015-09-03 00:00:00 -0400", "snippet": "In some applications, it’s important to display date and time information in the user’s expected time zone. One option is storing time zone preference information as part of a user’s account information. If this is not possible or you choose not to do so, if it’s a web application you can rely on the current time zone information from the user’s web browser.If you’re using ASP.NET, this can be done via Razor views using RimDev.AspNetBrowserLocale.First, install the RimDev.AspNetBrowserLocale NuGet package.Next, assuming you’re using the Razor view engine, initialize as follows in _Layout.cshtml or another file:@Html.InitializeLocaleDateTime()Now, you can use it from Razor:@{ var myDateTime = new DateTime(2015, 1, 1);}@Html.BrowserDisplay(myDateTime)JavaScript is used to display the information in the user’s time zone. Also, you’ll need a @using RimDev.AspNetBrowserLocale or equivalent for the above code examples.Hopefully this helps you solve this problem, in cases where you don’t want to collect or store the user’s preferred time zone information." }, { "title": "Sending Silverpop Transact XML Messages with C#", "url": "/sending-silverpop-transact-xml-messages-with-csharp/", "categories": "", "tags": "silverpop, transact, email, csharp", "date": "2015-05-06 00:00:00 -0400", "snippet": "Silverpop is an marketing service that includes email marketing – both for sending normal email campaign messages, as well as sending transactional emails. This transactional emailing is referred to as Silverpop Transact, which has an XML API available. These transactional messages could be sent as the result of a web application action, such as a new user registration.You can communicate directly with the XML API yourself. Alternatively, you can use the open source Silverpop .NET API wrapper, which is demonstrated below.It’s available on NuGet! Simply Install-Package silverpop-dotnet-api.Show me the code!// Initialize the client.// This method requires configuration to be set.var client = TransactClient.CreateUsingConfiguration();// Create a simple message.var message = TransactMessage.Create( 123, // TODO: Change this to be your campaign id! TransactMessageRecipient.Create(\"user@example.com\");// Send the message using the client.// This uses async/await.// Synchronous operations are available too.var response = await client.SendMessageAsync(message);Messages can also include personalization tags, for adding dynamic content to email messages.Note: SendMessage/SendMessageAsync are methods for sending messages to 1-10 recipients. For larger numbers of recipients use SendMessageBatch/SendMessageBatchAsync.In closingUsing the Silverpop .NET API should get you moving quickly, without having to understand the details of working with the Silverpop Transact XML API. It’s open source software, too!Note: The Silverpop .NET API is not sponsored by or affiliated with Silverpop.Disclosure: I’m a contributor for the Silverpop .NET API, developed for and to support Ritter Insurance Marketing." }, { "title": "Trimming Down the Visual Studio 2013 UI", "url": "/trimming-down-the-visual-studio-2013-ui/", "categories": "", "tags": "visual-studio, ui", "date": "2015-04-21 00:00:00 -0400", "snippet": "Here’s Visual Studio Community 2013 General settings defaults:If you want to simplify the UI, it’s possible to turn it into this:InstructionsHere’s how to achieve the trimmed down screenshot above: First, reset your settings with the Web Development (Code Only) settings. Tools -&gt; Import and Export Settings… -&gt; Reset all settings -&gt; No, just reset settings, overwriting my current settings (or, save your existing settings if you’d like…) -&gt; Web Development (Code Only) If you prefer having Solution Explorer always visible, now is an OK time to Un-Auto Hide it. Next, hide the menu bar. Install the Hide Main Menu Visual Studio extension. You can press Alt to reveal the menu bar when necessary. Now, you can download and import this settings file with Tools -&gt; Import and Export Settings…, or you can do the following: Tools -&gt; Options -&gt; Environment -&gt; General -&gt; Deselect “Show status bar” You’ll lose the ability to see the build status indicator on the status bar. You can view the build progress in the Output window, which you can open quickly with Ctrl+Alt+o. Tools -&gt; Options -&gt; Text Editor -&gt; General -&gt; Deselect “Selector margin” Tools -&gt; Options -&gt; Text Editor -&gt; General -&gt; Deselect “Indicator margin” You’ll lose the ability to click the indicator margin area to set and remove breakpoints. You can press F9 as a substitute, or right click -&gt; Breakpoint context menu item. Tools -&gt; Options -&gt; Text Editor -&gt; All Languages -&gt; General -&gt; Deselect “Navigation bar” Tools -&gt; Options -&gt; Text Editor -&gt; All Languages -&gt; Scroll Bars -&gt; Deselect “Show annotations over vertical scroll bar” Tools -&gt; Options -&gt; Text Editor -&gt; C# -&gt; Advanced -&gt; Deselect “Enter outlining mode when files open” Final notesAt this point you may want to perform your own customizations. You may prefer to enable line numbers, or have your test runner always visible.As new items like Error List appear during usage, you can decide what to do with them.Also, if you prefer having items visible that use significant space, consider using Full Screen as a view that doesn’t contain those items when you need additional real estate. You can toggle it with Shift+Alt+Enter or with View -&gt; Full Screen on the main menu." }, { "title": "2014: A Personal Recap", "url": "/2014-a-personal-recap/", "categories": "", "tags": "2014, recap", "date": "2014-12-31 00:00:00 -0500", "snippet": "I’ve had a great year with some great accomplishments, including: Started speaking in public! I delivered a talk at NYC Code Camp, two at Harrisburg .Net Code Camp, and one at BarCamp Harrisburg (the BarCamp is an unconference, but, I had prepared materials. And, attendees.). Started blogging. You’re reading it. Initial committer and currently core contributor for two .NET open source projects. Thanks to Ritter Insurance Marketing for supporting this work and OSS! Authored my first Chocolatey package: keepass-keepasshttp. It’s getting downloads too! Bought a house – it’s very nice. Worshipped with a good amount of improvisational violin. Tried jalapeño wine at a Renaissance Faire. I’m not a fan.This year was about me stretching myself and doing some new things. Next year I expect a similar list of items, but bigger and better.2014, you’ve been amazing, but I’m excited to see what 2015 has to offer!" }, { "title": "Exporting All Tickets from Zendesk", "url": "/exporting-all-tickets-from-zendesk/", "categories": "", "tags": "zendesk", "date": "2014-12-11 00:00:00 -0500", "snippet": "Zendesk is a web based customer service application that includes tickets.As a Zendesk customer, you can typically interact only with tickets that are not archived. Archiving takes place 120 days after the ticket has been closed, which is problematic for viewing all of your tickets and for creating reports requiring long running historical data.The SolutionThe ZenDesk Ticket Incremental Exporter can help! It orchestrates retrieval of all of your Zendesk tickets, enabling you to incrementally download only changes since your last retrieval.The first run may take considerable time depending upon the total number of tickets to retrieve. Subsequent runs, however, will only retrieve tickets created or modified since the previous run.The tool can generate a CSV file, which can be used with a spreadsheet application (or, anything that can use a CSV) to inspect the tickets or perform any desired analysis.Why the special tool?Downloading all tickets using the Zendesk API is non-trivial, as you need to make a separate request for each batch of tickets. Also, the Incremental Tickets API used for this is rate limited to one request per minute and only permits 1,000 tickets per request (at the time of this writing).How do I get started?Browse over to the ritterim/zendesk-ticket-exporter GitHub repository for installation and usage information.AcknowledgementSpecial thanks to Ritter Insurance Marketing for supporting open source software and giving me the opportunity to begin construction of this tool!" }, { "title": "Scary Words in Programming: Eventual Consistency", "url": "/scary-words-in-programming-eventual-consistency/", "categories": "", "tags": "eventual, consistency", "date": "2014-10-31 00:00:00 -0400", "snippet": "Pam Selle has been writing blog posts on Scary Words in Programming. This is my contribution on eventual consistency. Eventual consistency is a consistency model used in distributed computing to achieve high availability that informally guarantees that, if no new updates are made to a given data item, eventually all accesses to that item will return the last updated value. – Wikipedia: Eventual consistencyEssentially, in a database or system utilizing eventual consistency, changes made to data may have an impactful delay associated with changes being reflected. In a NoSQL document database, like RavenDB, the time between writing and reading the updated data from an index could be trivial or significant, depending upon the use case.Eventual consistency itself isn’t terribly complex or difficult to understand. But, it can be scary when implemented in ways that do not meet with user expectations. Users looking at data and seeing their changes not reflected can cause alarm. Developers should consider what users will see and do immediately after editing data in an application or website utilizing eventual consistency to avoid confusion and panic.If you choose to utilize eventual consistency, you’ll want to understand the implications of this decision and ideally shield your users from the concept, where possible." }, { "title": "Tips for the Aspiring or First Time Technical Speaker", "url": "/tips-for-the-aspiring-or-first-time-technical-speaker/", "categories": "", "tags": "speaking, presenting", "date": "2014-10-02 00:00:00 -0400", "snippet": "First of all, if you have an upcoming speaking engagement, congratulations on being selected!If you don’t have a speaking engagement yet, no worries. Now is a great low pressure time to learn, grow, and get comfortable speaking.Rehearse out loudDeliver your talk aloud. Don’t simply think about it, vocalize it.If you’re learning to play guitar, you need to learn to “keep up” and not stop to orient your fingers. Do the same with speaking. If you make a mistake, recover. Don’t simply start over or give yourself too much thinking time. During your out-loud rehearsal proceed as though you have an audience, mistakes and all.Spot opportunities for improvement while you’re rehearsing audibly. Frequent um’s, accidentally skipping or rushing through content, etc. are items you can work on, if necessary.Rehearse aloud until you are speaking comfortably, confidently, and fluently.Follow your own styleIf you’re a rockstar, bring a guitar, crank it up, and jump into the audience (actually, don’t do any of those things). If you’re a natural comedian, tell a joke. If you’re a storyteller, perhaps open with a story of personal experience. Be genuine. Be yourself.Focus on the engaged audienceTry to avoid focusing on that person and derailing your own train of thought. Ignore those who aren’t interested and don’t let it get you down. Instead, focus your attention and delivery on those who are fully engaged.Equipment concernsAsk about the presenting equipment available to you. If you intend to use a projector, find out what inputs are available (if you don’t already know). If it’s VGA only, ensure you have A Way To VGA ™.DemosIf you plan to demo, pre-zoom your text. Know how to zoom in and out or adjust font sizes in applications used in demos. Learn to use magnification.Be prepared if a demo goes completely and totally wrong. Have a working finished product you can fall back on, in case things don’t go as planned.You don’t have to know everythingIf someone asks a question and you don’t know the answer, no worries. Simply offer to connect with the individual later. If it’s a small group with a conversation feel, perhaps an audience member knows the answer and would like to share.Relax and have funDuring your talk, you’re the star of the show! Things may not go perfectly, but they’ll likely go better if you’re having a great time. Enjoy your talkDurationInMinutes minutes in the spotlight!WrapupIf there’s a speaker dinner or event, go! You’ve earned the right to attend. Celebrate!Also, speaker badges make you feel amazing." }, { "title": "Quickstart for Contributing to a GitHub Repository with a Development Branch", "url": "/quickstart-for-contributing-to-a-github-repository-with-a-development-branch/", "categories": "", "tags": "git, github", "date": "2014-09-29 00:00:00 -0400", "snippet": "Are you an eager GitHub user who wants to contribute to a repository that uses a development branch, but aren’t familiar with getting started without committing to master? No worries, here we go!Getting startedFirst, fork the kendaleiv/development-branch-master-as-default repository (or, substitute a repository of your choice). In this repository the master branch is set as the default branch.Next, run the following:&gt; git clone git@github.com:your-username-here/development-branch-master-as-default.git&gt; cd .\\development-branch-master-as-default&gt; git branchGit will only setup the GitHub defined default branch locally during the clone process. If git branch reports your current branch is development (the starred branch is the current branch), you’re good to start committing against development. If that is the case, you may want to set the upstream remote at this time, too.If your current branch is master, run the following:&gt; git remote add upstream git@github.com:kendaleiv/development-branch-development-as-default.git&gt; git fetch upstream&gt; git checkout -b development upstream/developmentYou should now be on the development branch locally.Further notesYou can discover what the default branch is for a repository by navigating to the repository on GitHub. If you navigate to kendaleiv/development-branch-master-as-default you’ll see the master branch is initially shown. For comparison, kendaleiv/development-branch-development-as-default initially shows the development branch.Also, remember to follow any guidelines for contributing, if they exist. This may include code style expectations, directions for submitting pull requests, and other guidance you should follow when contributing." }, { "title": "Build and Deploy Node.js With Travis CI and Heroku", "url": "/build-and-deploy-nodejs-with-travis-ci-and-heroku/", "categories": "", "tags": "nodejs, travis-ci, heroku, continuous-integration, deployment", "date": "2014-09-19 00:00:00 -0400", "snippet": "Travis CI is a cloud hosted continuous integration service that works with GitHub repositories for supported languages. Heroku is a cloud hosted platform as a service, enabling you to deploy applications without creating or maintaining server infrastructure for supported languages.If you have an application you wish to use for this exercise already in a GitHub repository, skip ahead to the Travis CI section. You may need to do some adjustments to your code to meet with Heroku expectations (specifically, using the requested port number from process.env.PORT).If you don’t have an existing Node.js application, and instead want to deploy a sample one, there’s one on the Heroku documentation you can use. On that page you’ll also find that you need to use npm init to create a valid package.json file, as well as information about engines if you want to control what version of Node.js is deployed (by default, it will deploy the latest stable version).Travis CIBrowse to https://travis-ci.org/ and flip the appropriate toggle switch, instructing Travis CI which GitHub repository to build. You’ll need to authorize Travis CI to interact with your GitHub account if you haven’t done so already.Travis CI uses a .travis.yml configuration file in the root of your repository. You’ll want to create this file before moving on. Here’s an example .travis.yml you can use, which will initiate a separate build for each of the specified versions of Node.js.Anytime you modify the .travis.yml file, you may wish to run it through a linter to verify its validity before you commit it. Travis CI has a web linter you can use.Now is a good time to ensure Travis CI builds. Push your code to the GitHub repo to get this process started. Then, navigate to Travis CI in a web browser and ensure everything looks good. Also, if you have automated tests, you may want to get these working now – but I’ll leave that to you.Deploy to Heroku from Travis CIYou’ll want to install the necessary prerequisites for this section. This includes the Heroku Toolbelt and the Travis CI Client.Run heroku login to setup the Heroku Toolbelt. If you’ve done this before, you may be able to skip this step.Next, create a new Heroku application with heroku create yourappname. For subsequent heroku commands interacting with this application – if you are outside your repository directory (or, you don’t have the Heroku Toolbelt configured for your local repository), you will need to append --app yourappname to your heroku commands to instruct the Heroku Toolbelt which application you wish to target.Run travis setup heroku. During this process you’ll need to provide your Heroku API key. Use the secure API key option. At no point should you commit or otherwise reveal your non-encrypted Heroku API key.Create the Heroku Procfile pointing to the main application entry point. Assuming web.js is your application entry point, use:web: node web.jsEverything should now be setup in your local repository. Commit and push your changes to GitHub, then wait for Travis CI to complete its operations.By default, Heroku doesn’t assign you a dyno to run your application on. To start one, run heroku ps:scale web=1. At the time of this post a single dyno can be ran for free.Now, browse to your application at the expected Heroku hosted address to see if it works. If everything appears working, good job and well done! If it doesn’t, don’t panic. Run heroku logs to try to find your issue.If no Node.js version information was explicitly specified, Heroku should deploy using the latest stable version of Node.js.Show off your buildAs a reward for a working cloud CI build, add the Travis CI build status badge to your GitHub README.md to show off the (hopefully passing) status of your Travis CI build. Simply add ![Travis CI Build Status](the_image_url_here) to the README.md to display the image. If you don’t have a README.md or equivalent file, now is a great time to create one!" }, { "title": "Refactor and Test jQuery/JS Code", "url": "/refactor-and-test-jquery-js-code/", "categories": "", "tags": "jquery, javascript, refactoring, testing", "date": "2014-08-27 00:00:00 -0400", "snippet": "jQuery is a JavaScript library many web developers utilize for writing concise and powerful browser code, as well as abstracting away browser differences and deficiencies. While jQuery may become less relevant over time as more modern and greenfield browsers are widely adopted, it is a real and present part of modern web development today.What jQuery does not provide, however, is any form of built-in programmatic structure. And, when applications are built with no thought as to organization, they can quickly become unmaintainable spaghetti code. Modifying the DOM, making ajax requests, and other program logic can be quick and easy to write using jQuery – doing what is necessary to Get The Job Done ™. However, when code is written with mixing of concerns, it can cause significant pain later in the form of a difficult to maintain codebase.Fortunately, many spaghetti code issues can be mitigated by following the single responsibility principle. Each function, module, component, etc. should ideally be fulfilling a single and specific concern. I’m not asserting follow this principle in the strictest sense, however, having this principle in mind may improve your code significantly.With mixed concerns:$.ajax(url).done(function (res) { var quote = $(res).find('[symbol=\"' + stockSymbol + '\"]'); var lastTradePrice = quote.find('LastTradePriceOnly').text(); // More code not related to getting the lastTradePrice});After refactor:(function (global, $) { 'use strict'; global.stockRetriever = global.stockRetriever || {}; global.stockRetriever.dataProvider = { getPrices: function (symbols) { return $.Deferred(function (dfd) { // Omitted for brevity: // Perform jQuery.ajax() or similar request and // resolve or reject the dfd. }).promise(); } };})(this, jQuery);stockRetriever.dataProvider.getPrices('MSFT') .done(function (res) { /* Code here */ });In the refactored code above the ajax request is encapsulated behind stockRetriever.dataProvider.getPrices(symbols) by creating a deferred using jQuery.Deferred() then resolving or rejecting it. We return a promise() from the deferred to mimic the same behavior as jQuery.ajax() and similar methods. Returning a promise() isn’t absolutely required, but it is good practice as it protects the deferred from other code resolving or rejecting it. Here’s the code if you’re feeling adventurous.Now is a great time to author some tests as follows:(Note: toBeADecimalNumber() is a Jasmine custom matcher)describe('dataProvider', function () { var dataProvider = stockRetriever.dataProvider; it('should return single price', function (specDone) { dataProvider .getPrices('MSFT') .done(function (res) { var item = res[0]; expect(item.symbol).toBe('MSFT'); expect(item.lastTradePrice).toBeADecimalNumber(); specDone(); }); }); it('should return multiple prices', function (specDone) { dataProvider .getPrices(['MSFT', 'GOOG']) .done(function (res) { var msft = res[0]; expect(msft.symbol).toBe('MSFT'); expect(msft.lastTradePrice).toBeADecimalNumber(); var goog = res[1]; expect(goog.symbol).toBe('GOOG'); expect(goog.lastTradePrice).toBeADecimalNumber(); specDone(); }); }); // \"should throw error for ...\" tests omitted});We can also test the code that calls dataProvider.getPrices, which you can view on GitHub.If you want to explore this further take a look at the kendaleiv/jquery-js-refactor GitHub repository. The initial commit is in a yet-to-be-refactored state, and each commit is (hopefully!) making it better." } ]
